{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T16:11:21.970271Z",
     "start_time": "2021-04-20T16:11:17.718992Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.dates as dates\n",
    "import matplotlib.animation as animation\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid, make_axes_locatable, host_subplot\n",
    "#from mpl_toolkits.basemap import Basemap\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "import os\n",
    "import pyPIPS.utils as utils\n",
    "import pyPIPS.thermolib as thermo\n",
    "import pyPIPS.DSDlib as dsd\n",
    "#import pyPIPS.disdrometer_module as dis\n",
    "import pyPIPS.plotmodule as PIPSplot\n",
    "#import pyPIPS.simulator as sim\n",
    "import pyPIPS.pips_io as pipsio\n",
    "import pyPIPS.PIPS as pips\n",
    "import pyPIPS.parsivel_params as pp\n",
    "import pyPIPS.parsivel_qc as pqc\n",
    "import pyPIPS.radarmodule as radar\n",
    "import pyPIPS.polarimetric as dualpol\n",
    "#from pyCRMtools.modules import plotmodule as plotmod\n",
    "from pyCRMtools.modules import utils as CRMutils\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import glob\n",
    "import numpy.random as random\n",
    "from scipy.stats import gamma, uniform\n",
    "from scipy.special import gamma as gammafunc\n",
    "from scipy import ndimage\n",
    "from scipy import interpolate\n",
    "from metpy.plots import StationPlot\n",
    "import metpy.calc as mpcalc\n",
    "from metpy.calc import wind_components\n",
    "from metpy.cbook import get_test_data\n",
    "from metpy.plots import StationPlot\n",
    "from metpy.plots.wx_symbols import current_weather, sky_cover\n",
    "from metpy.units import units\n",
    "from scipy.signal import medfilt2d\n",
    "import pyart\n",
    "import cartopy.crs as ccrs\n",
    "from IPython.display import HTML\n",
    "from io import StringIO\n",
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T16:11:24.052477Z",
     "start_time": "2021-04-20T16:11:23.921311Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function definitions\n",
    "def readsound(path,stype,rmqc=False):\n",
    "    \"\"\"Reads in a sounding from a text file and returns a pandas dataframe. Accepts various formats.\"\"\"\n",
    "    if(stype == 'ESC'):\n",
    "        return readESC(path)\n",
    "    elif(stype == 'CM1'):\n",
    "        return readCM1(path)\n",
    "    elif(stype == 'sharppy'):\n",
    "        return readsharppy(path)\n",
    "    else:\n",
    "        print(\"Sounding type not supported\")\n",
    "        return\n",
    "    \n",
    "def readCM1(path):\n",
    "    \"\"\"Reads in a sounding in CM1 (or COMMAS) input sounding format\"\"\"\n",
    "    col_names = ['height','potential temperature','water vapor mixing ratio','u_wind','v_wind']\n",
    "    with open(path) as f:\n",
    "        header = f.readline().strip().split()\n",
    "    psfc = float(header[0])\n",
    "    thetasfc = float(header[1])\n",
    "    qvsfc = float(header[2])\n",
    "    # Check whether mixing ratio is in g/kg or kg/kg and convert to kg/kg if needed\n",
    "    qvscale = 1.\n",
    "    if qvsfc > 0.1:\n",
    "        qvscale = 1.e-3\n",
    "    qvsfc = qvsfc*qvscale\n",
    "    df = pd.read_csv(path, skiprows=1, names=col_names, delim_whitespace=True)\n",
    "    df['water vapor mixing ratio'] = df['water vapor mixing ratio'].values*qvscale\n",
    "    # Check that height is in m, in some cases it's in km and needs to be converted\n",
    "    # z = df['height'].values\n",
    "    # If the final height is < 100. assume that the units are km and convert to m\n",
    "    if(df['height'].values[-1] < 100.):\n",
    "        df['height'] = df['height'].values*1000.\n",
    "    # Compute pressure\n",
    "    p = compute_pressure(df['height'].values, df['potential temperature'].values, \n",
    "                         df['water vapor mixing ratio'].values, psfc, qvsfc, thetasfc)\n",
    "    df['pressure'] = p\n",
    "    # Compute temperature\n",
    "    df['temperature'] = thermo.calT(df['pressure'].values*100., df['potential temperature'].values)-273.15\n",
    "    # Compute water vapor specific humidity and dewpoint\n",
    "    wv = df['water vapor mixing ratio'].values\n",
    "    df['water vapor specific humidity'] = wv/(1.+wv)\n",
    "    df['dewpoint'] = thermo.calTd(df['pressure'].values*100., df['water vapor specific humidity'].values)-273.15\n",
    "    return df\n",
    "\n",
    "def readESC(sounding_path, interpnan=True, handle=False):\n",
    "    \"\"\"\n",
    "    Reads in a sounding in ESC format from a provided file path or handle (can be a StringIO object or an open\n",
    "    file handle)\n",
    "    \"\"\"\n",
    "    col_names = ['pressure','temperature','dewpoint','u_wind','v_wind','speed','direction','height',\n",
    "                 'Qp_code','Qt_code','Qrh_code','Qu_code','Qv_code']\n",
    "    # First read the file and extract the field widths from the 14th header line\n",
    "    if not handle:\n",
    "        f = open(sounding_path, 'r')\n",
    "    else:\n",
    "        f = sounding_path\n",
    "\n",
    "    # Read in the header and extract some metadata from it\n",
    "    dummy = f.readline()\n",
    "    dummy = f.readline()\n",
    "    header2 = f.readline().strip().split(':')\n",
    "    # Read next header line and extract station id and wmo number from it (if it exists)\n",
    "    staid_wmo_str = header2[1]\n",
    "    if ' / ' in staid_wmo_str:\n",
    "        staid_wmo = staid_wmo_str.strip().split(' / ')\n",
    "        staid = staid_wmo[0][1:4]\n",
    "        wmo = int(staid_wmo[1])\n",
    "    else:\n",
    "        if '. ' in staid_wmo_str:\n",
    "            staid = staid_wmo_str.replace('. ', '').strip()[:4]\n",
    "        else:\n",
    "            staid = staid_wmo_str.strip()[:4]\n",
    "            staid = staid.replace(\" \", \"\")\n",
    "        wmo = 99999\n",
    "    print(staid)\n",
    "    # Read the next header line and extract the location information from it\n",
    "    header3 = f.readline().strip().split(':')\n",
    "    location = header3[1].strip().split(',')\n",
    "    print(location)\n",
    "    lon = float(location[2])\n",
    "    lat = float(location[3])\n",
    "    elev = float(location[4])\n",
    "    # Read the next header line and extract the time information from it\n",
    "    header4 = f.readline().strip()[31:].lstrip()   \n",
    "    sounding_datetime = datetime.strptime(header4, '%Y, %m, %d, %H:%M:%S')\n",
    "    \n",
    "    # Now read and dump the rest of the header\n",
    "    for i in range(9):\n",
    "        f.readline()\n",
    "    \n",
    "    # Except for the last header line, which is used to determine the widths of the fields\n",
    "    line = f.readline().strip().split()\n",
    "    fw = [len(field)+1 for field in line]\n",
    "\n",
    "    # Now read the file into the dataframe, using the extracted field widths\n",
    "    df = pd.read_fwf(f, usecols=[1, 2, 3, 5, 6, 7, 8, 14, 15, 16, 17, 18, 19],\n",
    "                     names=col_names, na_values=['99999.0', '9999.0', '999.0'], widths=fw)\n",
    "    \n",
    "    # For some reason, need to convert all the columns to floating point here, as some get interpreted as strings\n",
    "    # when they shouldn't be...\n",
    "    # print(df['pressure'], df['temperature'])\n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].astype(float)\n",
    "    \n",
    "    # Drop rows where height or pressure is NaN. TODO: Can't remember why I have to use reset_index(drop=True). \n",
    "    # Figure this out.\n",
    "    df = df.dropna(subset=('height', 'pressure')).reset_index(drop=True)\n",
    "    # Set the height as the index so we can use it as weights to interpolate other columns across NaN\n",
    "    df = df.set_index('height')\n",
    "    df['height'] = df.index\n",
    "    \n",
    "    if interpnan:\n",
    "        # First convert direction and speed to u, v components\n",
    "        df['u'], df['v'] = mpcalc.wind_components(df['speed'].values*units('m/s'),\n",
    "                                                      df['direction'].values*units.degrees)\n",
    "        # Now interpolate\n",
    "        df = df.interpolate(method='values')\n",
    "        # Finally recompute direction and speed from u, v components\n",
    "        df['speed'] = mpcalc.wind_speed(df['u'].values*units('m/s'), df['v'].values*units('m/s'))\n",
    "        df['direction'] = mpcalc.wind_direction(df['u'].values*units('m/s'), df['v'].values*units('m/s'))\n",
    "    else:\n",
    "        # Drop any rows with all NaN values for T, Td, winds\n",
    "        df = df.dropna(subset=('temperature', 'dewpoint', 'direction', 'speed',\n",
    "                               'u_wind', 'v_wind'), how='all').reset_index(drop=True)\n",
    "    \n",
    "    df = df[(df.Qp_code == 1.0) & (df.Qt_code == 1.0) & (df.Qrh_code == 1.0) & (df.Qu_code == 1.0) & \n",
    "            (df.Qv_code == 1.0)]\n",
    "\n",
    "    nlines = df.count()['pressure']\n",
    "    \n",
    "    if not handle:\n",
    "        f.close()\n",
    "    \n",
    "    snd_metadata = {\n",
    "        'sounding_datetime': sounding_datetime,\n",
    "        'lat': lat,\n",
    "        'lon': lon,\n",
    "        'selev': elev,\n",
    "        'staid': staid,\n",
    "        'wmo': wmo,\n",
    "        'nlevs': nlines,\n",
    "        'staid_long': staid_wmo_str\n",
    "    }\n",
    "    \n",
    "    return snd_metadata, df\n",
    "\n",
    "\n",
    "def readsharppy(path):\n",
    "    \"\"\"Reads in a sounding in sharppy format\"\"\"\n",
    "        ## read in the file\n",
    "    f = open(path, 'r')\n",
    "    lines = f.read()\n",
    "    data = np.array([l.strip() for l in lines.split('\\n')])\n",
    "    f.close()\n",
    "\n",
    "    ## necessary index points\n",
    "    title_idx = np.where( data == '%TITLE%')[0][0]\n",
    "    start_idx = np.where( data == '%RAW%' )[0][0] + 1\n",
    "    finish_idx = np.where( data == '%END%')[0][0]\n",
    "    \n",
    "    ## create the plot title\n",
    "    data_header = data[title_idx + 1].split()\n",
    "    location = data_header[0]\n",
    "    time = data_header[1][:11]\n",
    "\n",
    "    ## put it all together for StringIO\n",
    "    full_data = '\\n'.join(data[start_idx : finish_idx][:])\n",
    "    sound_data = StringIO( full_data )\n",
    "\n",
    "    ## read the data into arrays\n",
    "    p, h, T, Td, wdir, wspd = np.genfromtxt( sound_data, delimiter=',', comments=\"%\", unpack=True )\n",
    "    \n",
    "    col_names = ['pressure','height','temperature','dewpoint','speed','direction']\n",
    "    data_dict = {key:value for (key,value) in zip(col_names,[p,h,T,Td,wspd,wdir])}\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(data_dict)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def roundPartial(value, resolution, decimals=4):\n",
    "    return np.around(np.round(value / resolution) * resolution, decimals=decimals)\n",
    "\n",
    "\n",
    "def rain_Brandes(D):\n",
    "    \"\"\"Given a range of diameters D, compute rain fall speed curve, a quartic polynomial\n",
    "       fit after Brandes et al. (2002).\"\"\"\n",
    "    \n",
    "    D_mm=D*1000. # get it to (mm)\n",
    "    \n",
    "    Vtr = -0.1021 + 4.932*D_mm - 0.9551*D_mm**2. + 0.07934*D_mm**3. - 0.002362*D_mm**4.\n",
    "    \n",
    "    return Vtr\n",
    "\n",
    "\n",
    "def cal_xf_tf(usm, vsm, vt, H, perturb_vt=False, sigma=0.1):\n",
    "    \"\"\"Computes final horizontal position and residence time (relative to starting position) of a raindrop\n",
    "       falling through a horizontally homogeneous layer H with terminal velocity vt and \n",
    "       storm releative mean wind given by (usm, vsm).\"\"\"\n",
    "    \n",
    "    if perturb_vt:\n",
    "        rng = np.random.default_rng()\n",
    "        vt_perts = sigma * rng.standard_normal(vt.size)\n",
    "        vt = vt + vt_perts\n",
    "    \n",
    "    tf = H / vt\n",
    "    xf = tf * usm\n",
    "    yf = tf * vsm\n",
    "    \n",
    "    return xf, yf, tf\n",
    "\n",
    "\n",
    "def mtokm(val,pos):\n",
    "    \"\"\"Convert m to km for formatting axes tick labels\"\"\"\n",
    "    val=val/1000.0\n",
    "    return '%i' % val\n",
    "\n",
    "def interp_pyart_grid_to_PIPS(grid_ds, PIPS_loc_list):\n",
    "    \"\"\"Interpolate pyart gridded radar to PIPS locations given an xarray dataset with pyart grid info and a list\n",
    "       of PIPS lats, lons, and altitudes. Returns lists of PIPS x, y, and z locations in radar grid coordinates\n",
    "       and a list of xarray Datasets for each grid variable interpolated to each PIPS location.\"\"\"\n",
    "\n",
    "    # TODO: instead of returning lists, maybe make the PIPS name a dimension of a new xarray Dataset\n",
    "    \n",
    "    ctrlat = float(grid_ds.origin_latitude)\n",
    "    ctrlon = float(grid_ds.origin_longitude)\n",
    "    print(ctrlat, ctrlon)\n",
    "\n",
    "    radar_at_PIPS_list = []\n",
    "    PIPS_x_list = []\n",
    "    PIPS_y_list = []\n",
    "    PIPS_z_list = []\n",
    "\n",
    "    for PIPS_loc in PIPS_locs:\n",
    "        PIPS_lat = PIPS_loc[0]\n",
    "        PIPS_lon = PIPS_loc[1]\n",
    "        PIPS_alt = PIPS_loc[2]\n",
    "        radar_alt = float(grid_ds.origin_altitude)\n",
    "        \n",
    "        PIPS_z = PIPS_alt - radar_alt\n",
    "        PIPS_z_list.append(PIPS_z)\n",
    "        # Use this function to get the x and y coords of the PIPS. Note that this will only be correct \n",
    "        # if the radar\n",
    "        # grid was created using the default pyart aeqd projection.\n",
    "        PIPS_x, PIPS_y = pyart.core.geographic_to_cartesian_aeqd(PIPS_lon, PIPS_lat, ctrlon, ctrlat)\n",
    "        PIPS_x = PIPS_x.squeeze().item()\n",
    "        PIPS_y = PIPS_y.squeeze().item()\n",
    "        PIPS_x_list.append(PIPS_x)\n",
    "        PIPS_y_list.append(PIPS_y)\n",
    "        print('PIPS lat, lon, alt: ', PIPS_lat, PIPS_lon, PIPS_alt)\n",
    "        print('PIPS x, y, z: ', PIPS_x, PIPS_y, PIPS_z)\n",
    "        radar_at_PIPS_ds = grid_ds.interp(x=PIPS_x, y=PIPS_y)\n",
    "        radar_at_PIPS_list.append(radar_at_PIPS_ds)\n",
    "\n",
    "    return PIPS_x_list, PIPS_y_list, PIPS_z_list, radar_at_PIPS_list\n",
    "\n",
    "# DTD: modified from pyart to use gate edges instead of centers\n",
    "def get_gate_area(radarobj, sweep, edges=True):\n",
    "    \"\"\"\n",
    "    Return the area of each gate in a sweep. Units of area will be the \n",
    "    same as those of the range variable, squared.\n",
    "\n",
    "    Assumptions:\n",
    "        1. Azimuth data is in degrees.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sweep : int\n",
    "        Sweep number to retrieve gate locations from, 0 based.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    area : 2D array of size (ngates - 1, nrays - 1)\n",
    "        Array containing the area (in m * m) of each gate in the sweep.\n",
    "\n",
    "    \"\"\"\n",
    "    s = radarobj.get_slice(sweep)\n",
    "    azimuths = radarobj.azimuth['data'][s]\n",
    "    ranges = radarobj.range['data']\n",
    "    if edges:\n",
    "        azimuths = pyart.core.transforms._interpolate_azimuth_edges(azimuths)\n",
    "        ranges = pyart.core.transforms._interpolate_range_edges(ranges)\n",
    "        \n",
    "    circular_area = np.pi * ranges ** 2\n",
    "    annular_area = np.diff(circular_area)\n",
    "\n",
    "    d_azimuths = np.diff(azimuths) / 360. # Fraction of a full circle\n",
    "    # The following fixes the problem where the azimuths are differenced across 0/360 degs\n",
    "    d_azimuths = np.where(d_azimuths < 0., d_azimuths + 1., d_azimuths)\n",
    "\n",
    "    dca, daz = np.meshgrid(annular_area,d_azimuths)\n",
    "\n",
    "    area = np.abs(dca * daz)\n",
    "    return area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in the radar data\n",
    "radar_name = 'KGWX'\n",
    "radar_type= 'NEXRAD'\n",
    "elev_angle = 0.5\n",
    "elev_str = str(elev_angle).replace('.', 'p')\n",
    "elev_tol = 0.1\n",
    "\n",
    "date = '0330'\n",
    "radar_start_datetimestamp = '20220330233252' # '20220330220424'\n",
    "radar_end_datetimestamp = '20220331011617' # '20220331035633'\n",
    "\n",
    "# Create datetime objects for start and end times\n",
    "datetime_start = datetime.strptime(radar_start_datetimestamp, '%Y%m%d%H%M%S')\n",
    "datetime_end = datetime.strptime(radar_end_datetimestamp, '%Y%m%d%H%M%S')\n",
    "\n",
    "radar_basedir = \\\n",
    "    '/Users/dawson29/Projects/PERiLS/obsdata/2022/NEXRAD/IOP2/KGWX'\n",
    "#radar_basedir = os.path.join(radar_basedir, '{}/{}'.format(date, radar_name[1:]))\n",
    "radar_input_dir = os.path.join(radar_basedir, 'extracted_sweeps')\n",
    "radar_output_dir = os.path.join(radar_basedir, 'extracted_sweeps')\n",
    "if not os.path.exists(radar_output_dir):\n",
    "    os.makedirs(radar_output_dir)\n",
    "\n",
    "radar_start_timestamp = datetime_start.strftime('%Y%m%d%H%M')\n",
    "radar_end_timestamp = datetime_end.strftime('%Y%m%d%H%M')\n",
    "\n",
    "radar_paths = glob.glob(radar_input_dir + f'/{radar_name}*el{elev_str}_filt_retr.nc')\n",
    "\n",
    "radar_paths = sorted(radar_paths)\n",
    "radar_input_list = []\n",
    "\n",
    "for path in radar_paths:\n",
    "    filename = os.path.basename(path)\n",
    "    file_timestamp = filename[4:19]\n",
    "    # print(file_timestamp)\n",
    "    file_datetime = datetime.strptime(file_timestamp, '%Y%m%d_%H%M%S')\n",
    "    if file_datetime >= datetime_start and file_datetime <= datetime_end:\n",
    "        radar_input_list.append(path)\n",
    "\n",
    "# Read in the individual sweeps\n",
    "radarobj_list = []\n",
    "for radar_path in radar_input_list:\n",
    "    print(f\"Reading {os.path.basename(radar_path)}\")\n",
    "    radarobj = pyart.io.read(radar_path)\n",
    "    radarobj_list.append(radarobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T16:11:31.587429Z",
     "start_time": "2021-04-20T16:11:31.508425Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_dir = os.path.join(radar_output_dir, 'plots')\n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T16:11:35.460163Z",
     "start_time": "2021-04-20T16:11:34.805917Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in PIPS data (just to get lat/lon for now)\n",
    "deployment = 'IOP2_033022'\n",
    "PIPS_list = ['PIPS1A', 'PIPS1B', 'PIPS2A', 'PIPS3B']\n",
    "PIPS_data_dir = '/Users/dawson29/Projects/PERiLS/obsdata/2022/PIPS_data/IOP2_033022/netcdf'\n",
    "\n",
    "PIPS_ds_list = []\n",
    "PIPS_locs = []\n",
    "\n",
    "for PIPS in PIPS_list:\n",
    "    PIPS_filename = 'parsivel_combined_{}_{}_60s.nc'.format(deployment, PIPS)\n",
    "    PIPS_filepath = os.path.join(PIPS_data_dir, PIPS_filename)\n",
    "    PIPS_ds = xr.load_dataset(PIPS_filepath)\n",
    "    PIPS_ds_list.append(PIPS_ds)\n",
    "    PIPS_loc = eval(PIPS_ds.location)\n",
    "    PIPS_locs.append(PIPS_loc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for PIPS_ds in PIPS_ds_list:\n",
    "    print(PIPS_ds[\"{}_beam_height_at_PIPS\".format(radar_name)].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T16:11:43.305214Z",
     "start_time": "2021-04-20T16:11:43.183556Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in sounding file to get low-level wind field and then derive storm-relative wind\n",
    "# Storm motion taken from subjective reflectivity tag tracking using GRLevel2\n",
    "# EDIT: don't need storm motion because it is implicitly handled in time-dependent trajectory model\n",
    "# ustorm = 12.51\n",
    "# vstorm = 12.95\n",
    "\n",
    "# EDIT: setting ustorm, vstorm to 0 to force ground-relative flow\n",
    "ustorm = 0.\n",
    "vstorm = 0.\n",
    "\n",
    "# For 04/30 case\n",
    "# sounding_dir = '/Users/dawson29/sshfs_mounts/depot/data/Projects/VORTEXSE/obsdata/2017/soundings/COMP5mb'\n",
    "# sounding_filename = 'Hollywood_201704301954.cls'\n",
    "\n",
    "# For 03/27 case\n",
    "# sounding_dir = '/Users/dawson29/Projects/plotsnd/notebooks/'\n",
    "# sounding_filename = '201703272100_3469_8600.sharppy'\n",
    "\n",
    "# For 03/25 case \n",
    "# sounding_dir = '/Volumes/scr_fast/Projects/VORTEXSE/obsdata/2017/soundings/'\n",
    "# sounding_filename = 'Courtland_1759.txt'\n",
    "\n",
    "# sounding_path = os.path.join(sounding_dir, sounding_filename)\n",
    "\n",
    "# For 04/30 case\n",
    "# sounding_metadata, sounding_df = readESC(sounding_path)\n",
    "\n",
    "# For PERiLS IOP2 UIUC SONDE4\n",
    "sounding_dir = '/Users/dawson29/sshfs_mounts/depot/data/Projects/PERiLS/obsdata/2022/non-radar_QC_Illinois/20220330_IOP02/SONDE4/sounding/L2'\n",
    "sounding_filename = 'SPC_20220330_SONDE4_2328.txt'\n",
    "sounding_path = os.path.join(sounding_dir, sounding_filename)\n",
    "\n",
    "sounding_df = readsharppy(sounding_path)\n",
    "wind_dir = sounding_df['direction'].values*units.degrees\n",
    "print(wind_dir)\n",
    "wind_speed_kts = sounding_df['speed'].values*units.knots \n",
    "wind_speed_ms = wind_speed_kts.to(units('m/s'))\n",
    "print(wind_speed_ms)\n",
    "u, v = wind_components(wind_speed_ms, wind_dir)\n",
    "print(u, v)\n",
    "\n",
    "# For 03/27 case\n",
    "#sounding_df = readsharppy(sounding_path)\n",
    "#wind_dir = sounding_df['direction'].values*units.degrees\n",
    "#print(wind_dir)\n",
    "#wind_speed_kts = sounding_df['speed'].values*units.knots \n",
    "#wind_speed_ms = wind_speed_kts.to(units('m/s'))\n",
    "#print(wind_speed_ms)\n",
    "#u, v = wind_components(wind_speed_ms, wind_dir)\n",
    "#print(u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find PIPS x, y location relative to radar by interpolating to its lat/lon point\n",
    "\n",
    "rlat = radarobj_list[0].latitude['data'][0]\n",
    "rlon = radarobj_list[0].longitude['data'][0]\n",
    "ralt = radarobj_list[0].altitude['data'][0]\n",
    "\n",
    "PIPS_x_list = []\n",
    "PIPS_y_list = []\n",
    "PIPS_z_list = []\n",
    "\n",
    "for PIPS_ds in PIPS_ds_list:\n",
    "    geo_loc_str = PIPS_ds.location\n",
    "    geo_loc = list(map(float, geo_loc_str.strip('()').split(',')))\n",
    "    rad_loc = radar.get_PIPS_loc_relative_to_radar(geo_loc, rlat, rlon, ralt)\n",
    "    PIPS_x_list.append(rad_loc[0])\n",
    "    PIPS_y_list.append(rad_loc[1])\n",
    "    PIPS_z_list.append(rad_loc[2])\n",
    "    \n",
    "PIPS_z_mean = np.array(PIPS_z_list).mean()\n",
    "print(PIPS_z_list)\n",
    "print(PIPS_z_mean)\n",
    "PIPS_x_arr = np.array(PIPS_x_list)\n",
    "PIPS_y_arr = np.array(PIPS_y_list)\n",
    "\n",
    "print(PIPS_x_arr, PIPS_y_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract within a small bounding box and for a limited time slice\n",
    "\n",
    "# For 04/30 case\n",
    "# tstart = '2017-04-30T20:00'\n",
    "# tstop = '2017-04-30T21:30'\n",
    "\n",
    "# For 03/27 case\n",
    "# tstart = '2017-03-27T19:30'\n",
    "# tstop = '2017-03-27T21:00'\n",
    "\n",
    "# For 03/25 case\n",
    "# tstart = '2017-03-25T17:05'\n",
    "# tstop = '2017-03-25T18:35'\n",
    "\n",
    "# For PERiLS IOP2 2022 case\n",
    "tstart = '2022-03-30T23:30'\n",
    "tstop = '2022-03-31T01:15'\n",
    "\n",
    "# Create datetime objects for start and end times\n",
    "datetime_tstart = datetime.strptime(tstart, '%Y-%m-%dT%H:%M')\n",
    "datetime_tend = datetime.strptime(tstop, '%Y-%m-%dT%H:%M')\n",
    "\n",
    "# Exclude data beyond a given range\n",
    "max_range = 100000.\n",
    "gate_range = radarobj_list[0].range['data']\n",
    "gate_range_2D = np.tile(gate_range, (radarobj_list[0].nrays, 1))\n",
    "print(gate_range_2D)\n",
    "\n",
    "radarobj_sublist = []\n",
    "radarpath_sublist = []\n",
    "for path, radarobj in zip(radar_input_list, radarobj_list):\n",
    "    filename = os.path.basename(path)\n",
    "    file_timestamp = filename[4:19]\n",
    "    # print(file_timestamp)\n",
    "    file_datetime = datetime.strptime(file_timestamp, '%Y%m%d_%H%M%S')\n",
    "    if file_datetime >= datetime_tstart and file_datetime <= datetime_tend:\n",
    "        print(os.path.basename(path))\n",
    "        radarpath_sublist.append(path)\n",
    "        for field in radarobj.fields:\n",
    "            gate_range = radarobj.range['data']\n",
    "            gate_range_2D = np.tile(gate_range, (radarobj.nrays, 1))\n",
    "            # print(gate_range_2D)\n",
    "            radarobj.fields[field]['data'] = np.ma.masked_where(gate_range_2D > max_range, \n",
    "                                                                radarobj.fields[field]['data'])\n",
    "        radarobj_sublist.append(radarobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T16:12:20.541944Z",
     "start_time": "2021-04-20T16:12:20.468320Z"
    }
   },
   "outputs": [],
   "source": [
    "ref_varname = 'REF'\n",
    "zdr_varname = 'ZDR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radarobj = radarobj_sublist[0]\n",
    "ppi_display = pyart.graph.RadarMapDisplay(radarobj)\n",
    "projection = ccrs.PlateCarree()\n",
    "fig, ax = plt.subplots(figsize=[10, 10], subplot_kw={'projection': projection})\n",
    "ppi_display.plot_ppi_map(field='REF', sweep=0, vmin=-10, vmax = 65, cmap='pyart_NWSRef', ax=ax)\n",
    "ppi_display.plot_range_rings([50, 100, 150], lw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T16:16:42.756631Z",
     "start_time": "2021-04-20T16:16:41.447824Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate ND for each grid point from gamma dist. parameters using Parsivel bins\n",
    "use_parsivel_bins = True\n",
    "bin_width = 0.1\n",
    "retr_suffix = 'Z01_4dB'\n",
    "\n",
    "p_avg_diameters = pp.parsivel_parameters['avg_diameter_bins_mm']\n",
    "p_min_diameters = pp.parsivel_parameters['min_diameter_bins_mm']\n",
    "p_max_diameters = pp.parsivel_parameters['max_diameter_bins_mm']\n",
    "\n",
    "if use_parsivel_bins:\n",
    "    avg_diameters = p_avg_diameters\n",
    "    min_diameters = p_min_diameters\n",
    "    max_diameters = p_max_diameters\n",
    "else:\n",
    "    min_diameters = np.arange(p_min_diameters[0], p_max_diameters[-1], bin_width)\n",
    "    max_diameters = min_diameters + bin_width\n",
    "    avg_diameters = 0.5*(min_diameters + max_diameters)\n",
    "\n",
    "avg_diameters = xr.DataArray(avg_diameters, coords = {'diameter': ('diameter_bin', avg_diameters)}, \n",
    "                             dims=['diameter_bin'])\n",
    "\n",
    "\n",
    "# Loop through the radar sweeps and create xarray DataArrays for each sweep time. Add to lists and then\n",
    "# concatenate the lists into a single DataArray along the time dimension\n",
    "\n",
    "# TODO: can probably simplify this if the gates don't change across times...\n",
    "gate_x_xr_list = []\n",
    "gate_y_xr_list = []\n",
    "gate_z_xr_list = []\n",
    "gate_area_xr_list = []\n",
    "lamda_xr_list = []\n",
    "alpha_xr_list = []\n",
    "N0_xr_list = []\n",
    "\n",
    "max_range_index = 392 # index of range beyond 100000. m\n",
    "\n",
    "for radarobj in radarobj_sublist:\n",
    "    sweep_time = pyart.graph.common.generate_radar_time_sweep(radarobj, 0)\n",
    "    print(\"Processing for \", sweep_time)\n",
    "    # Get the x, y, and z coordinates of the gates\n",
    "    gate_x, gate_y, gate_z = radarobj.get_gate_x_y_z(0)\n",
    "    \n",
    "    new_range = radarobj.range['data'][None, :max_range_index+1]\n",
    "\n",
    "    gate_x = gate_x[:, :max_range_index+1]\n",
    "    gate_y = gate_y[:, :max_range_index+1]\n",
    "    gate_z = gate_z[:, :max_range_index+1]\n",
    "    \n",
    "    \n",
    "#     gate_x_xr = xr.DataArray(gate_x[None, ...], coords = {'time': ('time', [sweep_time]),\n",
    "#                                                           'azimuth': ('azimuth', radarobj.azimuth['data']),\n",
    "#                                                           'range': ('range', radarobj.range['data'])},\n",
    "#                              dims=['time', 'azimuth', 'range'])\n",
    "    \n",
    "    gate_x_xr = xr.DataArray(gate_x[None, ...], \n",
    "                             coords = {'time': ('time', [sweep_time]),\n",
    "                                       'azimuth_cor': (('time', 'azimuth'), radarobj.azimuth['data'][None, ...]),\n",
    "                                       'range_cor': (('time', 'range'), new_range)},\n",
    "                             dims=['time', 'azimuth', 'range'])\n",
    "    \n",
    "    gate_y_xr = xr.DataArray(gate_y[None, ...], \n",
    "                             coords = {'time': ('time', [sweep_time]),\n",
    "                                       'azimuth_cor': (('time', 'azimuth'), radarobj.azimuth['data'][None, ...]),\n",
    "                                       'range_cor': (('time', 'range'), new_range)},\n",
    "                             dims=['time', 'azimuth', 'range'])\n",
    "    gate_z_xr = xr.DataArray(gate_z[None, ...], \n",
    "                             coords = {'time': ('time', [sweep_time]),\n",
    "                                       'azimuth_cor': (('time', 'azimuth'), radarobj.azimuth['data'][None, ...]),\n",
    "                                       'range_cor': (('time', 'range'), new_range)},\n",
    "                             dims=['time', 'azimuth', 'range'])\n",
    "\n",
    "    # Also get area of each gate. We'll need it later\n",
    "    gate_area = get_gate_area(radarobj, 0)\n",
    "    \n",
    "    gate_area = gate_area[:, :max_range_index+1]\n",
    "\n",
    "    gate_area_xr = xr.DataArray(gate_area[None, ...], \n",
    "                             coords = {'time': ('time', [sweep_time]),\n",
    "                                       'azimuth_cor': (('time', 'azimuth'), radarobj.azimuth['data'][None, ...]),\n",
    "                                       'range_cor': (('time', 'range'), new_range)},\n",
    "                             dims=['time', 'azimuth', 'range'])\n",
    "\n",
    "\n",
    "    lamda = radarobj.fields[f'lamda_{retr_suffix}']['data'] * 1000. # get to m^-1\n",
    "    alpha = radarobj.fields[f'mu_{retr_suffix}']['data']\n",
    "    N0 = radarobj.fields[f'N0_{retr_suffix}']['data'] * 1000**(1 + alpha) # get to m^-4\n",
    "    \n",
    "    lamda = lamda[:, :max_range_index+1]\n",
    "    alpha = alpha[:, :max_range_index+1]\n",
    "    N0 = N0[:, :max_range_index+1]\n",
    "\n",
    "    lamda_xr = xr.DataArray(lamda[None, ...], coords = {'time': ('time', [sweep_time]),\n",
    "                                                        'y': (('azimuth', 'range'), gate_y_xr.squeeze().data),\n",
    "                                                        'x': (('azimuth', 'range'), gate_x_xr.squeeze().data)},\n",
    "                            dims=['time', 'azimuth', 'range'])\n",
    "    alpha_xr = xr.DataArray(alpha[None, ...], coords = {'time': ('time', [sweep_time]),\n",
    "                                                        'y': (('azimuth', 'range'), gate_y_xr.squeeze().data),\n",
    "                                                        'x': (('azimuth', 'range'), gate_x_xr.squeeze().data)},\n",
    "                            dims=['time', 'azimuth', 'range'])\n",
    "\n",
    "    N0_xr = xr.DataArray(N0[None, ...], coords = {'time': ('time', [sweep_time]),\n",
    "                                                  'y': (('azimuth', 'range'), gate_y_xr.squeeze().data),\n",
    "                                                  'x': (('azimuth', 'range'), gate_x_xr.squeeze().data)},\n",
    "                         dims=['time', 'azimuth', 'range'])\n",
    "\n",
    "    gate_x_xr_list.append(gate_x_xr)\n",
    "    gate_y_xr_list.append(gate_y_xr)\n",
    "    gate_z_xr_list.append(gate_z_xr)\n",
    "    gate_area_xr_list.append(gate_area_xr)\n",
    "    lamda_xr_list.append(lamda_xr)\n",
    "    alpha_xr_list.append(alpha_xr)\n",
    "    N0_xr_list.append(N0_xr)\n",
    "    \n",
    "\n",
    "# Concatenate above lists\n",
    "gate_x_xr_full = xr.concat(gate_x_xr_list, dim='time')\n",
    "gate_y_xr_full = xr.concat(gate_y_xr_list, dim='time')\n",
    "gate_z_xr_full = xr.concat(gate_z_xr_list, dim='time')\n",
    "gate_area_xr_full = xr.concat(gate_area_xr_list, dim='time')\n",
    "lamda_xr_full = xr.concat(lamda_xr_list, dim='time')\n",
    "alpha_xr_full = xr.concat(alpha_xr_list, dim='time')\n",
    "N0_xr_full = xr.concat(N0_xr_list, dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ND = dsd.calc_binned_DSD_from_params(N0_xr_full, lamda_xr_full, \n",
    "                                     alpha_xr_full, avg_diameters) * 1.e-3 # Get to m^-3 mm^-1\n",
    "ND.coords['max_diameter'] = ('diameter_bin', max_diameters)\n",
    "ND.coords['min_diameter'] = ('diameter_bin', min_diameters)\n",
    "# Add time_seconds coordinate to ND, representing the number of seconds since the first sweep\n",
    "# For the gridded data, this was done in the advect_correct_and_interp notebook, but since we aren't doing \n",
    "# that for this workflow (yet) we need to do it here.\n",
    "time_seconds = (ND['time'] - ND['time'][0]) / np.timedelta64(1, 's')\n",
    "print(time_seconds)\n",
    "ND.coords['time_seconds'] = ('time', time_seconds.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new time intervals for interpolation\n",
    "interval = 30\n",
    "freq = f'{interval:d}S'\n",
    "\n",
    "new_times = xr.date_range(tstart, tstop, freq=freq)\n",
    "new_tstart = tstart\n",
    "for time in new_times:\n",
    "    if time >= ND['time'][0]:\n",
    "        new_tstart = time\n",
    "        break\n",
    "print(new_tstart)\n",
    "\n",
    "keep_indices = np.where(new_times >= new_tstart)\n",
    "print(keep_indices)\n",
    "print(new_times[keep_indices])\n",
    "\n",
    "new_times = new_times[keep_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now interpolate to regular time intervals using the nearest *previous* times\n",
    "ND = ND.interp(time=new_times, method='previous', kwargs={'fill_value': 'extrapolate'})\n",
    "time_seconds = (ND['time'] - ND['time'][0]) / np.timedelta64(1, 's')\n",
    "print(time_seconds)\n",
    "ND.coords['time_seconds'] = ('time', time_seconds.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ND = ND.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_x_xr_full = gate_x_xr_full.interp(time=new_times, method='previous', \n",
    "                                       kwargs={'fill_value': 'extrapolate'})\n",
    "gate_y_xr_full = gate_y_xr_full.interp(time=new_times, method='previous', \n",
    "                                       kwargs={'fill_value': 'extrapolate'})\n",
    "gate_z_xr_full = gate_z_xr_full.interp(time=new_times, method='previous', \n",
    "                                       kwargs={'fill_value': 'extrapolate'})\n",
    "gate_area_xr_full = gate_area_xr_full.interp(time=new_times, method='previous', \n",
    "                                             kwargs={'fill_value': 'extrapolate'})\n",
    "lamda_xr_full = lamda_xr_full.interp(time=new_times, method='previous', \n",
    "                                     kwargs={'fill_value': 'extrapolate'})\n",
    "alpha_xr_full = alpha_xr_full.interp(time=new_times, method='previous', \n",
    "                                     kwargs={'fill_value': 'extrapolate'})\n",
    "N0_xr_full = N0_xr_full.interp(time=new_times, method='previous', \n",
    "                               kwargs={'fill_value': 'extrapolate'})\n",
    "\n",
    "gate_x_xr_full = gate_x_xr_full.astype(np.float32)\n",
    "gate_y_xr_full = gate_y_xr_full.astype(np.float32)\n",
    "gate_z_xr_full = gate_z_xr_full.astype(np.float32)\n",
    "gate_area_xr_full = gate_area_xr_full.astype(np.float32)\n",
    "lamda_xr_full = lamda_xr_full.astype(np.float32)\n",
    "alpha_xr_full = alpha_xr_full.astype(np.float32)\n",
    "N0_xr_full = N0_xr_full.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_ds = ND.to_dataset(name='ND').copy()\n",
    "radar_ds['gate_x'] = gate_x_xr_full\n",
    "radar_ds['gate_y'] = gate_y_xr_full\n",
    "radar_ds['gate_z'] = gate_z_xr_full\n",
    "radar_ds['gate_area'] = gate_area_xr_full\n",
    "radar_ds['lamda'] = lamda_xr_full\n",
    "radar_ds['alpha'] = alpha_xr_full\n",
    "radar_ds['N0'] = N0_xr_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_ds = radar_ds.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump to file \n",
    "\n",
    "radar_ds.attrs['sorting_layer_top_elev'] = elev_angle\n",
    "\n",
    "tstart_datetime = datetime.strptime(tstart, '%Y-%m-%dT%H:%M')\n",
    "tstop_datetime = datetime.strptime(tstop, '%Y-%m-%dT%H:%M')\n",
    "\n",
    "tstart_out = tstart_datetime.strftime('%Y%m%d%H%M')\n",
    "tstop_out = tstop_datetime.strftime('%Y%m%d%H%M')\n",
    "\n",
    "radar_DSD_filename = '{}_{}_{}_el{}_radar_DSDs_{}.nc'.format(radar_name, tstart_out, tstop_out, elev_str, freq)\n",
    "print(radar_DSD_filename)\n",
    "radar_DSD_path = os.path.join(radar_output_dir, radar_DSD_filename)\n",
    "print(radar_DSD_path)\n",
    "radar_ds.to_netcdf(radar_DSD_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plt = gate_x_xr_full[0]\n",
    "y_plt = gate_y_xr_full[0]\n",
    "area_plt = gate_area_xr_full[0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ci = ax.pcolormesh(x_plt, y_plt, area_plt, cmap='plasma')\n",
    "cbarintv = 1.\n",
    "cbarlevels = ticker.MultipleLocator(base=cbarintv)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "fig.colorbar(ci, orientation='vertical', cax=cax)\n",
    "# fig.colorbar(ci, orientation='vertical', ticks=cbarlevels, cax=cax)\n",
    "cax.set_ylabel('ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(gate_z_xr_full[0].sel(range_cor=100000., method='nearest'))\n",
    "temp = gate_z_xr_full[0, 0].set_xindex('range_cor')\n",
    "np.where(temp.range_cor>=100000.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# COPIED FROM PREVIOUS PRECIP_TRAJ SCRIPT\n",
    "# Set up grid for precip trajectories\n",
    "# TODO: need to reconcile fact that elevation of sounding site, PIPS sites, *and* radar site are not generally\n",
    "# the same. For now, just take into account the difference in elevations between the PIPS and radar and assume\n",
    "# sounding is at same elevation as average elevation of PIPS\n",
    "\n",
    "# For 04/30 case (FIX THIS to be more general, but we are using ESC format for 04/30, and sharppy for 03/27)\n",
    "# height_AGL_snd = sounding_df['height'] - sounding_metadata['selev']\n",
    "# For 03/27 case\n",
    "# height_AGL_snd = sounding_df['height']\n",
    "# print(gridded_radar_subgrid['x'])\n",
    "\n",
    "# For PERiLS IOP2 2022 case\n",
    "height_AGL_snd = sounding_df['height']\n",
    "print(height_AGL_snd)\n",
    "# Subtract mean height difference of PIPS relative to the radar from the radar beam heights \n",
    "# (relative to the radar) to get radar beam height relative to the (average of) the PIPS altitudes\n",
    "beam_height = gate_z_xr_full - PIPS_z_mean\n",
    "\n",
    "beam_height = beam_height.astype(np.float32)\n",
    "\n",
    "# print(beam_height)\n",
    "# print(beam_height[0].dropna(dim='azimuth'))\n",
    "# print(beam_height[1].dropna(dim='azimuth'))\n",
    "# Compute the layer mean winds between the mean PIPS altitude and the radar beam for each gate\n",
    "# First set an appropriate max height for the wind profile. Use the beam height above PIPS at 100 km range\n",
    "# rounded to nearest m\n",
    "max_height = np.around(beam_height[0, 0].set_xindex('range_cor').sel(range_cor=100000., method='nearest'))\n",
    "max_height = max_height.astype(np.float32)\n",
    "print(max_height)\n",
    "# Create a range of heights at 1-m grid spacing. Probably overkill, but oh well\n",
    "dz = 1.\n",
    "new_heights = np.arange(0., max_height + dz, dz)\n",
    "new_heights = new_heights.astype(np.float32)\n",
    "\n",
    "# Interpolate sounding u, v to new regularly spaced heights\n",
    "# For 04/30 case (FIXME)\n",
    "# u_snd = sounding_df['u'].values\n",
    "# For 03/27 case/PERiLS IOP2\n",
    "u_snd = u\n",
    "f = interpolate.interp1d(height_AGL_snd, u_snd, bounds_error=False, fill_value=(u_snd[0], u_snd[-1]))\n",
    "ug = f(new_heights)\n",
    "ug = ug.astype(np.float32)\n",
    "\n",
    "# For 04/30 case (FIXME)\n",
    "# v_snd = sounding_df['v'].values\n",
    "# For 03/27 case / PERiLS IOP2\n",
    "v_snd = v\n",
    "f = interpolate.interp1d(height_AGL_snd, v_snd, bounds_error=False, fill_value=(v_snd[0], v_snd[-1]))\n",
    "vg = f(new_heights)\n",
    "vg = vg.astype(np.float32)\n",
    "# Layer mean wind below radar gate for each gate\n",
    "# First get an array representing the index of each height in new_heights closest to and just above\n",
    "# the beam height for each gate. Use np.searchsorted for this\n",
    "beam_height_indices = np.searchsorted(new_heights, beam_height)\n",
    "print(beam_height_indices.shape)\n",
    "# Then use these indices to compute the mean wind components up to each height for each gate\n",
    "# Based on https://stackoverflow.com/questions/65778535/average-of-a-3d-numpy-slice-based-on-2d-arrays?rq=3\n",
    "\n",
    "# This way is very memory intensive since it is trying to do it for all spatial points for all times all at\n",
    "# once. So, I modified it to iterate through each time individually and then concatenate everything at the end\n",
    "\n",
    "# height_indices = np.arange(new_heights.shape[0])[:, None, None, None]\n",
    "# print(height_indices.shape)\n",
    "# print(ug.shape)\n",
    "\n",
    "# ugm = np.where((height_indices < beam_height_indices), ug[:, None, None, None], 0).sum(0) / beam_height_indices\n",
    "# vgm = np.where((height_indices < beam_height_indices), vg[:, None, None, None], 0).sum(0) / beam_height_indices\n",
    "\n",
    "height_indices = np.arange(new_heights.shape[0])[:, None, None]\n",
    "print(height_indices.shape)\n",
    "\n",
    "ugm_list = []\n",
    "vgm_list = []\n",
    "\n",
    "for t, beam_height_indices_1t in enumerate(beam_height_indices):\n",
    "    print(\"Working on time \", t)\n",
    "    ugm_1t = np.where((height_indices < beam_height_indices_1t), \n",
    "                      ug[:, None, None], 0).sum(0) / beam_height_indices_1t\n",
    "    vgm_1t = np.where((height_indices < beam_height_indices_1t), \n",
    "                      vg[:, None, None], 0).sum(0) / beam_height_indices_1t\n",
    "\n",
    "    ugm_list.append(ugm_1t)\n",
    "    vgm_list.append(vgm_1t)\n",
    "\n",
    "ugm = np.array(ugm_list)\n",
    "vgm = np.array(vgm_list)\n",
    "    \n",
    "\n",
    "# # # Storm-relative winds\n",
    "# # usr = ug - ustorm\n",
    "# # vsr = vg - vstorm\n",
    "\n",
    "# # # Layer-mean storm-relative winds\n",
    "# # usm = np.mean(usr)\n",
    "# # vsm = np.mean(vsr)\n",
    "# # print(usm, vsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T16:17:09.942329Z",
     "start_time": "2021-04-20T16:17:01.602272Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Truncate diameter range to less than 9 mm\n",
    "D_max = 9.\n",
    "D_range_full = ND['diameter'].values\n",
    "D_max_ind = np.searchsorted(D_range_full, D_max)\n",
    "D_range = D_range_full[:D_max_ind]\n",
    "print(D_range)\n",
    "ND_trunc = ND.isel(diameter_bin=slice(0, D_max_ind))\n",
    "\n",
    "ND_trunc = ND_trunc.astype(np.float32)\n",
    "\n",
    "# Compute range of terminal velocities from Brandes relation\n",
    "vt_range = rain_Brandes(D_range / 1000.)\n",
    "vt_range = vt_range.astype(np.float32)\n",
    "print(vt_range)\n",
    "\n",
    "# Set dimensions back from lat/lon to y/x for ND_trunc\n",
    "# ND_trunc = ND_trunc.swap_dims({'lon': 'x', 'lat': 'y'})\n",
    "ND_trunc = ND_trunc.swap_dims({'time': 'time_seconds'})\n",
    "\n",
    "\n",
    "# Interpolate ND to a finer grid\n",
    "# Set up grid of locations\n",
    "x_coords = ND_trunc['x']\n",
    "y_coords = ND_trunc['y']\n",
    "t_coords = ND_trunc['time_seconds']\n",
    "\n",
    "refinement_factor = 4\n",
    "time_refinement_factor = 1\n",
    "\n",
    "# new_x_coords = np.linspace(x_coords.x[0], x_coords.x[-1], (x_coords.sizes['x'] - 1) * refinement_factor + 1)\n",
    "# new_y_coords = np.linspace(y_coords.y[0], y_coords.y[-1], (y_coords.sizes['y'] - 1) * refinement_factor + 1)\n",
    "# new_t_coords = np.linspace(t_coords.time_seconds[0], t_coords.time_seconds[-1], \n",
    "#                            (t_coords.sizes['time_seconds'] - 1) * time_refinement_factor + 1)\n",
    "\n",
    "new_x_coords = x_coords.astype(np.float32)\n",
    "new_y_coords = y_coords.astype(np.float32)\n",
    "new_t_coords = t_coords.astype(np.float32)\n",
    "\n",
    "\n",
    "# ND_trunc = ND_trunc.interp(x=new_x_coords, y=new_y_coords, time_seconds=new_t_coords)\n",
    "\n",
    "x_grid, y_grid, t_grid = xr.broadcast(ND_trunc['x'], ND_trunc['y'], ND_trunc['time_seconds'])\n",
    "\n",
    "x_grid = x_grid.astype(np.float32)\n",
    "y_grid = y_grid.astype(np.float32)\n",
    "t_grid = t_grid.astype(np.float32)\n",
    "\n",
    "x_flat = x_grid.stack(loc=['time_seconds', 'azimuth', 'range']).values\n",
    "y_flat = y_grid.stack(loc=['time_seconds', 'azimuth', 'range']).values\n",
    "t_flat = t_grid.stack(loc=['time_seconds', 'azimuth', 'range']).values\n",
    "\n",
    "# Compute horizontal deviations of drops and residence time at bottom of layer for each grid point and drop size\n",
    "# TODO: Generalize this for spatially varying velocity field. Would require a numerical trajectory integration\n",
    "# TODO: Update this to perturb the terminal velocities for all grid points (already modified the function above)\n",
    "\n",
    "# Modified to loop through individual times to save memory\n",
    "\n",
    "# xf, yf, tf = cal_xf_tf(ugm[None, ...], vgm[None, ...], vt_range[:, None, None], beam_height.values[None, ...])\n",
    "# xf, yf, tf = cal_xf_tf(ugm[:, :, :, None], vgm[:, :, :, None], vt_range[None, None, None, :], \n",
    "#                        beam_height.values[:, :, :, None])\n",
    "\n",
    "xf_list = []\n",
    "yf_list = []\n",
    "tf_list = []\n",
    "\n",
    "ugm = ugm.astype(np.float32)\n",
    "vgm = vgm.astype(np.float32)\n",
    "\n",
    "for t, ugm_1t, vgm_1t, beam_height_1t in zip(range(len(t_coords)), ugm, vgm, beam_height.values):\n",
    "    print(\"Working on time \", t)\n",
    "    xf_1t, yf_1t, tf_1t = cal_xf_tf(ugm_1t[:, :, None], vgm_1t[:, :, None], vt_range[None, None, :], \n",
    "                                    beam_height_1t[:, :, None])\n",
    "    xf_list.append(xf_1t)\n",
    "    yf_list.append(yf_1t)\n",
    "    tf_list.append(tf_1t)\n",
    "\n",
    "xf = np.array(xf_list)\n",
    "yf = np.array(yf_list)\n",
    "tf = np.array(tf_list)\n",
    "\n",
    "print(xf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Do some reshaping to get everyting to line up properly\n",
    "# xf = np.array([xf] * len(t_coords))\n",
    "xf = xf.reshape(-1, xf.shape[-1])\n",
    "# yf = np.array([yf] * len(t_coords))\n",
    "yf = yf.reshape(-1, yf.shape[-1])\n",
    "# tf = np.array([tf] * len(t_coords))\n",
    "tf = tf.reshape(-1, tf.shape[-1])\n",
    "\n",
    "print(\"Here!\")\n",
    "\n",
    "x_flat_f = x_flat[:, np.newaxis] + xf\n",
    "y_flat_f = y_flat[:, np.newaxis] + yf\n",
    "\n",
    "# Create array of times corresponding to each initial time for trajectory endpoints as a function of diameter\n",
    "t_flat_f = t_flat[:, np.newaxis] + tf\n",
    "\n",
    "print(\"Here! 2\")\n",
    "\n",
    "x_flat_f = x_flat_f.T\n",
    "y_flat_f = y_flat_f.T\n",
    "t_flat_f = t_flat_f.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perturb the endpoints a bit in space and time\n",
    "# xpertscale = 20. # m\n",
    "# tpertscale = 1. # s\n",
    "# rng = np.random.default_rng()\n",
    "# xpert = xpertscale * rng.standard_normal(size=x_flat_f.shape, dtype=np.float32)\n",
    "# ypert = xpertscale * rng.standard_normal(size=y_flat_f.shape, dtype=np.float32)\n",
    "# tpert = tpertscale * rng.standard_normal(size=t_flat_f.shape, dtype=np.float32)\n",
    "\n",
    "print(\"Here!\")\n",
    "\n",
    "# This is taking a *ton* of memory. Disable for right now.\n",
    "\n",
    "# xpert = np.where(xpert < -10.*xpertscale, 0., xpert)\n",
    "# xpert = np.where(xpert > 10.*xpertscale, 0., xpert)\n",
    "# ypert = np.where(ypert < -10.*xpertscale, 0., ypert)\n",
    "# ypert = np.where(ypert > 10.*xpertscale, 0., ypert)\n",
    "# tpert = np.where(tpert < -10.*tpertscale, 0., tpert)\n",
    "# tpert = np.where(tpert > 10.*tpertscale, 0., tpert)\n",
    "\n",
    "print(\"Here! 2\")\n",
    "# x_flat_f = x_flat_f + xpert\n",
    "# y_flat_f = y_flat_f + ypert\n",
    "# t_flat_f = t_flat_f + tpert\n",
    "\n",
    "ND_trunc = ND_trunc.transpose('diameter_bin', 'time_seconds', 'azimuth', 'range')\n",
    "print(x_flat_f.shape, y_flat_f.shape, t_flat_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(t_coords))\n",
    "print(x_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xf.shape)\n",
    "\n",
    "ntimes = len(t_coords)\n",
    "nazim = len(x_coords['azimuth'])\n",
    "nrange = len(x_coords['range'])\n",
    "print(ntimes, nazim, nrange)\n",
    "\n",
    "print(xf.reshape((ntimes, nazim, nrange, -1)).shape)\n",
    "\n",
    "xf_thin = xf.reshape((ntimes, nazim, nrange, -1))[0]\n",
    "yf_thin = yf.reshape((ntimes, nazim, nrange, -1))[0]\n",
    "\n",
    "print(xf_thin.shape)\n",
    "\n",
    "xf_thin = xf_thin[::5, ::5, :]\n",
    "yf_thin = yf_thin[::5, ::5, :]\n",
    "\n",
    "print(xf_thin.shape)\n",
    "\n",
    "xf_thin = xf_thin.reshape(-1, xf_thin.shape[-1])\n",
    "yf_thin = yf_thin.reshape(-1, yf_thin.shape[-1])\n",
    "\n",
    "print(xf_thin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T14:34:33.622095Z",
     "start_time": "2021-04-09T14:34:32.461940Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test plot of endpoints\n",
    "\n",
    "x_grid_plt = x_grid.sel(time_seconds=0.)\n",
    "y_grid_plt = y_grid.sel(time_seconds=0.)\n",
    "\n",
    "x_grid_plt = x_grid_plt.thin(indexers=5)\n",
    "y_grid_plt = y_grid_plt.thin(indexers=5)\n",
    "\n",
    "x_flat_plt = x_grid_plt.stack(loc=['azimuth', 'range']).values\n",
    "y_flat_plt = y_grid_plt.stack(loc=['azimuth', 'range']).values\n",
    "\n",
    "print(x_flat_plt.shape)\n",
    "\n",
    "# Compute horizontal deviations of drops and residence time at bottom of layer for each grid point and drop size\n",
    "# TODO: Generalize this for spatially varying velocity field. Would require a numerical trajectory integration\n",
    "# TODO: Update this to perturb the terminal velocities for all grid points (already modified the function above)\n",
    "x_flat_f_plt = x_flat_plt[:, np.newaxis] + xf_thin\n",
    "y_flat_f_plt = y_flat_plt[:, np.newaxis] + yf_thin\n",
    "\n",
    "print(x_flat_f_plt.shape)\n",
    "\n",
    "Nt = ND_trunc.sel(time_seconds=0.).sum(dim='diameter_bin')\n",
    "Nt = Nt.thin(indexers=5)\n",
    "print(Nt)\n",
    "Nt_flat = Nt.values.flatten()\n",
    "indices = np.where(Nt_flat > 0.)[0]\n",
    "# indices = indices[::100]\n",
    "\n",
    "print(ND_trunc['diameter'].values)\n",
    "\n",
    "x_flat_f_0 = x_flat_f_plt[:, 8]\n",
    "y_flat_f_0 = y_flat_f_plt[:, 8]\n",
    "x_flat_f_1 = x_flat_f_plt[:, 16]\n",
    "y_flat_f_1 = y_flat_f_plt[:, 16]\n",
    "\n",
    "# gridded_radar_subgrid_2 = gridded_radar_subgrid.swap_dims({'time': 'time_seconds'})\n",
    "\n",
    "radarobj = radarobj_sublist[0]\n",
    "\n",
    "var_to_plot = radarobj.fields[f'{ref_varname}_filtered']['data']\n",
    "# var_to_plot = vgm\n",
    "\n",
    "# print(ugm.shape)\n",
    "xplt, yplt, zplt = radarobj.get_gate_x_y_z(0, edges=True)\n",
    "\n",
    "\n",
    "# var_da = gridded_radar_subgrid_2[f'{zdr_varname}_masked'].sel(time_seconds=0.)\n",
    "# xplt = gridded_radar_subgrid_2.coords[\"x\"]\n",
    "# yplt = gridded_radar_subgrid_2.coords[\"y\"]\n",
    "# print(xplt[0], xplt[-1], yplt[0], yplt[-1])\n",
    "\n",
    "clevels =np.arange(0., 4., 0.1)\n",
    "norm = cm.colors.Normalize(vmin=0., vmax=4.)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "print(xplt.shape, yplt.shape, var_to_plot.shape)\n",
    "# ci = ax.contourf(xplt, yplt, var_da.squeeze(), levels=clevels, cmap='plasma', norm=norm)\n",
    "ci = ax.pcolormesh(xplt, yplt, var_to_plot, cmap='pyart_NWSRef')\n",
    "ax.scatter(x_flat_plt[indices], y_flat_plt[indices], marker='*', color='k', label='Initial points', alpha=0.5)\n",
    "ax.scatter(x_flat_f_0[indices], y_flat_f_0[indices], marker='x', color='b', label='Endpoint (D = 1 mm)', alpha=0.5)\n",
    "ax.scatter(x_flat_f_1[indices], y_flat_f_1[indices], marker='o', color='g', label='Endpoint (D = 3.25 mm)', alpha=0.5)\n",
    "\n",
    "cbarintv = 1.\n",
    "cbarlevels = ticker.MultipleLocator(base=cbarintv)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "fig.colorbar(ci, orientation='vertical', ticks=cbarlevels, cax=cax)\n",
    "cax.set_ylabel('dB')\n",
    "\n",
    "# ax.set_xlim(-26000., -16000.)\n",
    "# ax.set_ylim(-40000., -30000.)\n",
    "# ax.set_xlim(80000., 120000.)\n",
    "# ax.set_ylim(60000., 100000.)\n",
    "# ax.set_xlim(-20000., 0.)\n",
    "# ax.set_ylim(-20000., 0.)\n",
    "\n",
    "ax.set_xlim(-50000., 50000.)\n",
    "ax.set_ylim(-50000., 50000.)\n",
    "\n",
    "formatter = ticker.FuncFormatter(mtokm)\n",
    "ax.xaxis.set_major_formatter(formatter)\n",
    "ax.yaxis.set_major_formatter(formatter)\n",
    "ax.set_xlabel('km')\n",
    "ax.set_ylabel('km')\n",
    "ax.legend()\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plot_filename = 'trajectory_endpoint_example.png'\n",
    "plot_filepath = os.path.join(plot_dir, plot_filename)\n",
    "fig.savefig(plot_filepath, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T16:17:23.431696Z",
     "start_time": "2021-04-20T16:17:23.350016Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set up 3D bins (2 for space, 1 for time) for bottom of sorting layer\n",
    "\n",
    "dx_bins = 500.\n",
    "area_bins = dx_bins**2.\n",
    "\n",
    "area_ratio = area_bins / gate_area_xr_full\n",
    "\n",
    "# xplt = gate_x_xr_full[0]\n",
    "# yplt = gate_y_xr_full[0]\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10,10))\n",
    "# ci = ax.pcolormesh(xplt, yplt, area_ratio[0], vmin=0, vmax=5, cmap='plasma')\n",
    "# cbarintv = 0.25\n",
    "# cbarlevels = ticker.MultipleLocator(base=cbarintv)\n",
    "# divider = make_axes_locatable(ax)\n",
    "# cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "# # fig.colorbar(ci, orientation='vertical', cax=cax)\n",
    "# fig.colorbar(ci, orientation='vertical', ticks=cbarlevels, cax=cax)\n",
    "# cax.set_ylabel('ratio')\n",
    "\n",
    "\n",
    "# dx_orig = new_x_coords[1] - new_x_coords[0]\n",
    "# print(\"dx (top), dx (bottom)\", dx_orig, dx_bins)\n",
    "# area_ratio = dx_bins**2. / dx_orig**2.\n",
    "# print(\"area ratio: \", area_ratio)\n",
    "# Get bounding box of bottom of domain\n",
    "# xmin = int(x_flat_f.min()) # -5000.\n",
    "# xmax = int(x_flat_f.max())\n",
    "# ymin = int(y_flat_f.min()) # -5000.\n",
    "# ymax = int(y_flat_f.max())\n",
    "\n",
    "# Or, just use the original bounds\n",
    "xmin = np.min(new_x_coords)\n",
    "xmax = np.max(new_x_coords)\n",
    "ymin = np.min(new_y_coords)\n",
    "ymax = np.max(new_y_coords)\n",
    "\n",
    "# Create bins for bottom of domain\n",
    "xbins = int((xmax-xmin)/dx_bins)\n",
    "ybins = int((ymax-ymin)/dx_bins)\n",
    "\n",
    "print(xbins)\n",
    "print(ybins)\n",
    "xmax = xmin+dx_bins*xbins # +5000.\n",
    "ymax = ymin+dx_bins*ybins # +5000.\n",
    "print(xmin, xmax, ymin, ymax)\n",
    "\n",
    "# Set up time bins\n",
    "tintv = 60.\n",
    "\n",
    "# Need to take into account the ratio of the original time interval at the top of the sorting layer and\n",
    "# that of the chosen time bins in order for the weights to work out correctly later when calling np.histogramdd\n",
    "tratio = tintv / interval\n",
    "print(\"Ratio of time bins (bottom vs top): \", tratio)\n",
    "\n",
    "# tmin = t_flat_f.min()\n",
    "# tmax = t_flat_f.max()\n",
    "# print(tmin, tmax)\n",
    "# tbins = int((tmax - tmin) / tintv)\n",
    "tmin = ND_trunc['time_seconds'][0].values\n",
    "tmax = ND_trunc['time_seconds'][-1].values + tintv\n",
    "tbins = int((tmax - tmin) / tintv)\n",
    "# tbins = ND_trunc['time_seconds'].values\n",
    "print(tmin, tmax, tbins)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ND_trunc.values.shape)\n",
    "print(x_flat_f.shape)\n",
    "print(ND_trunc.values.reshape(ND_trunc.shape[0], -1).shape)\n",
    "print(area_ratio.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(area_ratio)\n",
    "print(ND_trunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T16:17:35.675116Z",
     "start_time": "2021-04-20T16:17:26.381655Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create 3D histogram (x, y, t) for number density for each diameter bin for drop trajectory endpoints\n",
    "\n",
    "ND_D_binned_list = []\n",
    "\n",
    "for i, ND_D in enumerate(ND_trunc):\n",
    "    # print(ND_D)\n",
    "    print(ND_D.shape)\n",
    "    print(\"Diameter: \", ND_D['diameter'].values)\n",
    "    \n",
    "    x = x_flat_f[i]\n",
    "    y = y_flat_f[i]\n",
    "    t = t_flat_f[i]\n",
    "    \n",
    "    print(x.shape, y.shape, t.shape)\n",
    "    \n",
    "    # Scale ND by ratio of surface bin area to original gate area as well as the ratio of the surface time bins\n",
    "    # to the radar time bins. This is to make sure the individual trajectory endpoints are weighted properly\n",
    "    # when creating the 3D histogram\n",
    "    # Also just operate on underlying numpy arrays to avoid mismatched\n",
    "    # coordinate shenanigans\n",
    "    ND_D_flat = ND_D.values.flatten()\n",
    "    area_ratio_flat = area_ratio.values.flatten()\n",
    "    # Create a mask to exclude all points with non-positive number concentration and area ratio\n",
    "    keep = np.where((ND_D_flat > 0.) & (area_ratio_flat > 0.))\n",
    "    ND_D_flat = ND_D_flat[keep]\n",
    "    area_ratio_flat = area_ratio_flat[keep]\n",
    "    # Scale by area ratio\n",
    "    ND_D_flat = ND_D_flat / area_ratio_flat\n",
    "    # Scale by time bin ratio\n",
    "    ND_D_flat = ND_D_flat / tratio\n",
    "    \n",
    "\n",
    "    x = x[keep]\n",
    "    y = y[keep]\n",
    "    t = t[keep]\n",
    "\n",
    "    print(x.shape, y.shape, t.shape)\n",
    "    coords = np.stack([x, y, t], axis=1)\n",
    "    print(coords.shape)\n",
    "    print(ND_D_flat.shape)\n",
    "    ND_D_binned, edges = np.histogramdd(coords,\n",
    "                                        bins=[xbins, ybins, tbins],\n",
    "                                        range=[[xmin, xmax], [ymin, ymax], [tmin, tmax]],\n",
    "                                        weights=ND_D_flat)\n",
    "    print(ND_D_binned.shape)\n",
    "    ND_D_binned_list.append(ND_D_binned)\n",
    "\n",
    "\n",
    "ND_D_binned = np.array(ND_D_binned_list)\n",
    "print(ND_D_binned.shape)\n",
    "\n",
    "# Now done above because the area ratio depends on the originating gate and is no longer constant\n",
    "# ND_D_binned = ND_D_binned / area_ratio\n",
    "xedges, yedges, tedges = edges\n",
    "# Shift times to start at 0 again\n",
    "tedges = tedges - tedges[0]\n",
    "start_time = ND['time'][0].values\n",
    "attrs = {\"units\": \"seconds since {}\".format(start_time)}\n",
    "\n",
    "ND_f_da = xr.DataArray(ND_D_binned,\n",
    "                       coords={\n",
    "                           \"diameter\": ND_trunc['diameter'], \n",
    "                           \"x\": xedges[:-1],\n",
    "                           \"y\": yedges[:-1],\n",
    "                           \"time_seconds\": tedges[:-1],\n",
    "                       },\n",
    "                       dims=[\"diameter_bin\", \"x\", \"y\", \"time_seconds\"])\n",
    "\n",
    "ND_f_da.coords[\"time\"] = (\"time_seconds\", ND_f_da[\"time_seconds\"].data, attrs)\n",
    "ND_f_ds = xr.decode_cf(ND_f_da.to_dataset(name='ND'))\n",
    "ND_f_ds = ND_f_ds.swap_dims({\"time_seconds\": \"time\"})\n",
    "ND_f_ds.coords['max_diameter'] = ('diameter_bin', max_diameters[:D_max_ind])\n",
    "ND_f_ds.coords['min_diameter'] = ('diameter_bin', min_diameters[:D_max_ind])\n",
    "ND_f_ds = ND_f_ds.transpose(\"time\", \"y\", \"x\", \"diameter_bin\")\n",
    "ND_f_ds = ND_f_ds.swap_dims({'diameter_bin': 'diameter'}) # Do this so we can use it for sel function. May\n",
    "                                                          # break other stuff so make sure to check later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ND_D_binned_list\n",
    "print(tedges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ND_f_ds = ND_f_ds.transpose('time', 'y', 'x', 'diameter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ND_f_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T14:41:47.603140Z",
     "start_time": "2021-04-09T14:41:42.438623Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate radar variables for new surface DSDs\n",
    "# Get polarimetric variables\n",
    "# This uses a *ton* of memory. Need to think of ways to lower the memory footprint\n",
    "# ND_f_ds = ND_f_ds.swap_dims({'diameter': 'diameter_bin'})\n",
    "dD = ND_f_ds['max_diameter'] - ND_f_ds['min_diameter']\n",
    "\n",
    "scatt_dir = '/Users/dawson29/Projects/pyPIPS/tmatrix/S-Band/'\n",
    "if use_parsivel_bins:\n",
    "    scatt_file = 'SCTT_RAIN_fw100.dat'\n",
    "else:\n",
    "    scatt_file = 'SCTT_RAIN_fw100_0p1_even.dat'\n",
    "    \n",
    "scatt_path = os.path.join(scatt_dir, scatt_file)\n",
    "dualpol_dict = dualpol.calpolrain_bulk_xr(10.7, scatt_path, ND_f_ds['ND'], dD, diameter_bin_name='diameter')\n",
    "print(dualpol_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_stats(PIPS_ds, PIPS_x, PIPS_y):\n",
    "    bias = (100. * (PIPS_ds[PIPS_y] - PIPS_ds[PIPS_x]).mean() / PIPS_ds[PIPS_x].mean()).values\n",
    "    cc = pd.DataFrame({'x': PIPS_ds[PIPS_x], 'y': PIPS_ds[PIPS_y]}).corr()\n",
    "    \n",
    "    return cc.iloc[0, 1], bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T14:54:02.585090Z",
     "start_time": "2021-04-09T14:54:02.496106Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combine sorted ND and polarimetric arrays together into a single dataset and dump to disk\n",
    "print(ND_f_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T14:54:04.833110Z",
     "start_time": "2021-04-09T14:54:04.759079Z"
    }
   },
   "outputs": [],
   "source": [
    "print(dualpol_dict['REF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "dualpol_dict['REF'][-1].plot(ax=ax)\n",
    "ax.set_xlim(-50000., 50000.)\n",
    "ax.set_ylim(-50000., 50000.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T14:54:09.264543Z",
     "start_time": "2021-04-09T14:54:09.189533Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted_ds = ND_f_ds.copy()\n",
    "sorted_ds['REF'] = dualpol_dict['REF']\n",
    "sorted_ds['ZDR'] = dualpol_dict['ZDR']\n",
    "print(sorted_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T14:54:32.512396Z",
     "start_time": "2021-04-09T14:54:28.418102Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dump to file \n",
    "\n",
    "# Add some metadata first\n",
    "# sorted_ds.attrs['orig_dx'] = dx_orig * refinement_factor\n",
    "# sorted_ds.attrs['top_dx'] = dx_orig\n",
    "sorted_ds.attrs['bottom_dx'] = dx_bins\n",
    "sorted_ds.attrs['sorting_layer_top'] = elev_angle\n",
    "sorted_ds.attrs['sorting_layer_bottom'] = 0.  # Generalize this later\n",
    "\n",
    "# print(sorted_ds)\n",
    "\n",
    "tstart_datetime = datetime.strptime(tstart, '%Y-%m-%dT%H:%M')\n",
    "tstop_datetime = datetime.strptime(tstop, '%Y-%m-%dT%H:%M')\n",
    "\n",
    "tstart_out = tstart_datetime.strftime('%Y%m%d%H%M')\n",
    "tstop_out = tstop_datetime.strftime('%Y%m%d%H%M')\n",
    "\n",
    "sorted_filename = '{}_{}_{}_el{}_{}_sorted.nc'.format(radar_name, tstart_out, tstop_out, elev_str, freq)\n",
    "print(sorted_filename)\n",
    "sorted_path = os.path.join(radar_output_dir, sorted_filename)\n",
    "print(sorted_path)\n",
    "sorted_ds.to_netcdf(sorted_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:pyPIPS]",
   "language": "python",
   "name": "conda-env-pyPIPS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
