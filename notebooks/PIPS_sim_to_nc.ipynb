{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T19:12:13.683916Z",
     "start_time": "2019-09-09T19:12:11.006485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## You are using the Python ARM Radar Toolkit (Py-ART), an open source\n",
      "## library for working with weather radar data. Py-ART is partly\n",
      "## supported by the U.S. Department of Energy as part of the Atmospheric\n",
      "## Radiation Measurement (ARM) Climate Research Facility, an Office of\n",
      "## Science user facility.\n",
      "##\n",
      "## If you use this software to prepare a publication, please cite:\n",
      "##\n",
      "##     JJ Helmus and SM Collis, JORS 2016, doi: 10.5334/jors.119\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dawson29/.conda/envs/cent7/5.1.0-py27/pyPIPS/lib/python3.7/site-packages/pyart/graph/cm.py:104: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if 'red' in spec:\n",
      "/home/dawson29/.conda/envs/cent7/5.1.0-py27/pyPIPS/lib/python3.7/site-packages/pyart/graph/cm_colorblind.py:32: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if 'red' in spec:\n"
     ]
    }
   ],
   "source": [
    "# PIPS_sim_to_nc.ipynb: creates intermediate netCDF files from model output and PIPS data for use in the\n",
    "# Parsivel simulator and other analyses/comparisons\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import inspect\n",
    "from datetime import datetime, timedelta\n",
    "import pytz as pytz\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.dates as dates\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid, make_axes_locatable, host_subplot\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import pyPIPS.utils as utils\n",
    "import pyPIPS.thermolib as thermo\n",
    "import pyPIPS.DSDlib as dsd\n",
    "import pyPIPS.disdrometer_module as dis\n",
    "import pyPIPS.plotmodule as PIPSplot\n",
    "import pyPIPS.simulator as sim\n",
    "import pyPIPS.radarmodule as pyPIPSradar\n",
    "import pyPIPS.PIPS as pips\n",
    "import pyPIPS.pips_io as pips_io\n",
    "from pyCRMtools.modules import plotmodule as plotmod\n",
    "from pyCRMtools.modules import utils as CRMutils\n",
    "from pyCRMtools.pycaps import arps_read\n",
    "from pyCRMtools.pycaps import pycaps_fields\n",
    "from pyCRMtools.pycaps import calvars_radar as radar\n",
    "from joblib import Parallel, delayed\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T19:12:16.291110Z",
     "start_time": "2019-09-09T19:12:16.225918Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define dictionaries, keyed by case date (i.e. '060509', '060709', '060909', '033116'), to store parameters related\n",
    "# to NEXRAD radar data, disdrometer data, and model output, respectively\n",
    "\n",
    "# Case we are looking at right now. Should only have to change this up here and then execute all the cells below\n",
    "# to generate the appropriate analysis\n",
    "casedate = '033116'\n",
    "\n",
    "# Import the file containing the dictionaries needed to gather the radar, disdrometer, and model data.\n",
    "sys.path.append('/depot/dawson29/data/Projects/vortexse_enkf_dsd_study/configs/2016_IOP3')\n",
    "\n",
    "from PIPSsim_1km_dicts_rice import *\n",
    "\n",
    "init_radar_dict = init_radar_dict[casedate]\n",
    "init_dis_dict = init_dis_dict[casedate]\n",
    "init_model_dict = init_model_dict[casedate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T19:12:19.103599Z",
     "start_time": "2019-09-09T19:12:19.037941Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up the model information\n",
    "modelname = 'ARPS'\n",
    "# From desired start and end times (UTC) get a range of datetimes and corresponding range of times in\n",
    "# seconds since model initial time\n",
    "timestamp_model_init = init_model_dict['timestamp_model_init']  # Start time of model corresponding to 0 s\n",
    "datetime_model_init = datetime.strptime(timestamp_model_init, '%Y%m%d%H%M%S')\n",
    "\n",
    "timestamp_start = init_model_dict['timestamp_model_start']  # Start time of desired time window\n",
    "timestamp_stop = init_model_dict['timestamp_model_stop']  # Stop time of desired time window\n",
    "datetime_start = datetime.strptime(timestamp_start, '%Y%m%d%H%M%S')\n",
    "datetime_stop = datetime.strptime(timestamp_stop, '%Y%m%d%H%M%S')\n",
    "tintv = init_model_dict['model_dt']  # Interval in seconds for model output\n",
    "tintv_mean = init_model_dict['model_dt_mean'] # Interval in seconds for ensemble mean analysis\n",
    "\n",
    "datetime_range = CRMutils.get_datetime_range(datetime_start, datetime_stop, tintv)\n",
    "trange = CRMutils.modeltimes_from_datetimes(datetime_range, datetime_start=datetime_model_init)\n",
    "\n",
    "datetime_range_mean = CRMutils.get_datetime_range(datetime_start, datetime_stop, tintv_mean)\n",
    "trange_mean = CRMutils.modeltimes_from_datetimes(datetime_range_mean, datetime_start=datetime_model_init)\n",
    "\n",
    "#basedir = '/Volumes/scr_fast/Projects/VORTEXSE/simulations/ARPS/2016_IOP3/3DVAR/1km0331163DVARCA00005min180_3km030015min540'\n",
    "filetype = 'history'\n",
    "fileformat = init_model_dict['fileformat']\n",
    "expname = '1km453x453_newse'\n",
    "basedir = init_model_dict['basedirname']\n",
    "num_members = 36\n",
    "nproc_x = 15\n",
    "nproc_y = 6\n",
    "\n",
    "# Tell the arps_read module what the processor numbers are.\n",
    "# Yes, I know this is a bad way to do this through globals. I'll fix it eventually.\n",
    "# arps_read.nproc_x = nproc_x\n",
    "# arps_read.nproc_y = nproc_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T19:13:02.475449Z",
     "start_time": "2019-09-09T19:12:21.136813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['nx', 'ny', 'nz', 'dx', 'dy', 'dz', 'x', 'y', 'z', 'zp', 'xs', 'ys', 'zs', 'zpagl', 'zsagl'])\n"
     ]
    }
   ],
   "source": [
    "# Load the ARPS grid\n",
    "# Get file path for grdbas file (note that call to read_grid handles the reading of the individual patches)\n",
    "# If the grdbas file doesn't exist, fall back to a history file\n",
    "member = 1 # 0 is for ensemble mean\n",
    "cycle = 'posterior'\n",
    "member_dir, member_prefix = sim.get_ARPS_member_dir_and_prefix(member, cycle)\n",
    "member_absdir = os.path.join(basedir, expname, member_dir)\n",
    "trailer = ''\n",
    "grdbas_path = arps_read.get_file_path(member_absdir, member_prefix, fileformat, filetype='grdbas')\n",
    "\n",
    "patch_x = 1\n",
    "patch_y = 1\n",
    "grdbas_path_test = arps_read.add_patch_number(grdbas_path, patch_x, patch_y)\n",
    "\n",
    "if not os.path.exists(grdbas_path_test):\n",
    "    print(\"grdbas file doesn't exist, trying a history file!\")\n",
    "    grdbas_path = arps_read.get_file_path(member_absdir, member_prefix, fileformat, time=model_trange_sec[0], \n",
    "                                          filetype='history')\n",
    "    grdbas_path_test = arps_read.add_patch_number(grdbas_path, patch_x, patch_y)\n",
    "\n",
    "    print(grdbas_path_test)\n",
    "    print(os.path.exists(grdbas_path_test))\n",
    "\n",
    "# Read in grid information\n",
    "grid_dict = arps_read.readarpsgrid(grdbas_path, nproc_x=nproc_x, nproc_y=nproc_y)\n",
    "print(grid_dict.keys())\n",
    "\n",
    "# Get map projection information and create a Basemap instance\n",
    "# TODO: convert to use cartopy!\n",
    "\n",
    "ctrlat, ctrlon, trulat1, trulat2, trulon = arps_read.readarpsmap(grdbas_path, nproc_x=nproc_x, nproc_y=nproc_y)\n",
    "\n",
    "dx = grid_dict['dx']\n",
    "dy = grid_dict['dy']\n",
    "nx = grid_dict['nx']\n",
    "ny = grid_dict['ny']\n",
    "\n",
    "mapwidth = nx * dx\n",
    "mapheight = ny * dy\n",
    "\n",
    "bgmap = Basemap(projection='lcc', width=mapwidth, height=mapheight, lat_1=trulat1,\n",
    "                lat_2=trulat2, lat_0=ctrlat, lon_0=ctrlon, resolution='h',\n",
    "                area_thresh=10., suppress_ticks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T19:13:04.819577Z",
     "start_time": "2019-09-09T19:13:02.681285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[195667.8426462494, 252256.91376130987, 225473.53498544852, 290574.924876145]\n",
      "197 254 226 293\n"
     ]
    }
   ],
   "source": [
    "# Find coordinates of PIPS stations in the model\n",
    "# Put the basemap instance into the grid_dict\n",
    "grid_dict['bgmap'] = bgmap\n",
    "dis_dict = sim.get_dis_locs_arps_real_grid(init_dis_dict, grid_dict)\n",
    "coord_array = np.array(dis_dict['dmodcrdlist'])\n",
    "dxlist = [i[0] for i in dis_dict['dmodloclist']]\n",
    "dylist = [i[1] for i in dis_dict['dmodloclist']]\n",
    "xc = grid_dict['xs']\n",
    "yc = grid_dict['ys']\n",
    "xe = grid_dict['x']\n",
    "ye = grid_dict['y']\n",
    "# Set model grid limits to center on the disdrometer locations \n",
    "\n",
    "Dxmin = min(dxlist)\n",
    "Dxmax = max(dxlist)\n",
    "Dymin = min(dylist)\n",
    "Dymax = max(dylist)\n",
    "gridlims = [Dxmin - 25000., Dxmax + 25000., Dymin - 25000., Dymax + 25000.]\n",
    "\n",
    "ibgn = np.searchsorted(xc, gridlims[0])\n",
    "iend = np.searchsorted(xc, gridlims[1]) + 1\n",
    "jbgn = np.searchsorted(yc, gridlims[2])\n",
    "jend = np.searchsorted(yc, gridlims[3]) + 1\n",
    "\n",
    "print(gridlims)\n",
    "print(ibgn, iend, jbgn, jend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T19:13:07.214308Z",
     "start_time": "2019-09-09T19:13:05.083425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['dis_dir', 'dis_types', 'dis_names', 'disfilenames', 'convfilenames', 'starttimes', 'stoptimes', 'interval', 'dgeoloclist', 'dmodloclist', 'dmodcrdlist'])\n",
      "Reading /depot/dawson29/data/Projects/VORTEXSE/obsdata/2016/PIPS/processed/IOP3/PIPS_1A_IOP_3_D1.txt\n",
      "GPS time: Thu Mar 31 22:12:03 2016, Logger time: Thu Mar 31 22:12:00 2016\n",
      "GPS Offset: 0:00:03\n",
      "Reading /depot/dawson29/data/Projects/VORTEXSE/obsdata/2016/PIPS/processed/IOP3/PIPS_1B_IOP_3_D1.txt\n",
      "GPS time: Thu Mar 31 22:01:33 2016, Logger time: Thu Mar 31 22:01:30 2016\n",
      "GPS Offset: 0:00:03\n",
      "Reading /depot/dawson29/data/Projects/VORTEXSE/obsdata/2016/PIPS/processed/IOP3/PIPS_2A_IOP_3_D1.txt\n",
      "GPS time: Thu Mar 31 22:25:03 2016, Logger time: Thu Mar 31 22:25:00 2016\n",
      "GPS Offset: 0:00:03\n",
      "Reading /depot/dawson29/data/Projects/VORTEXSE/obsdata/2016/PIPS/processed/IOP3/PIPS_2B_IOP_3_D1.txt\n",
      "GPS time: Thu Mar 31 21:48:23 2016, Logger time: Thu Mar 31 21:48:20 2016\n",
      "GPS Offset: 0:00:03\n"
     ]
    }
   ],
   "source": [
    "# Read in PIPS data\n",
    "print(dis_dict.keys())\n",
    "\n",
    "dis_dir = dis_dict['dis_dir']\n",
    "dis_filenames = dis_dict['disfilenames']\n",
    "dis_names = dis_dict['dis_names']\n",
    "\n",
    "conv_df_dict = {}\n",
    "parsivel_df_dict = {}\n",
    "vd_matrix_da_dict = {}\n",
    "\n",
    "for dis_filename, dis_name in zip(dis_filenames, dis_names):\n",
    "    dis_filepath = os.path.join(dis_dir, dis_filename)\n",
    "    print(\"Reading {}\".format(dis_filepath))\n",
    "    conv_df, parsivel_df, vd_matrix_da = pips_io.read_PIPS(dis_filepath, starttimestamp=timestamp_start,\n",
    "                                                           stoptimestamp=timestamp_stop)\n",
    "    # Calculate some additional thermodynamic quantities and add to the conventional data DataFrame\n",
    "    conv_df = pips.calc_thermo(conv_df)\n",
    "    conv_df_dict[dis_name] = conv_df\n",
    "    parsivel_df_dict[dis_name] = parsivel_df\n",
    "    vd_matrix_da_dict[dis_name] = vd_matrix_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T19:24:19.861276Z",
     "start_time": "2019-09-09T19:13:32.889601Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in needed fields from the ARPS history dumps\n",
    "# Just for one member now. Later will parallelize this and put it in a script.\n",
    "# and interpolate them to the PIPS locations, building up a time series.\n",
    "\n",
    "def read_ensemble(f, member_list, f_args=None, f_kwargs=None, iterate_over='member_list',\n",
    "                  process_parallel=True, n_jobs=5, verbose=0):\n",
    "    \"\"\"\n",
    "    Reads multiple runs either in serial or parallel (using joblib). TODO. This is duplicating some\n",
    "    functionality written by T. Supinie for pycaps (util.run_concurrently). Maybe just use that\n",
    "    instead?\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : callable\n",
    "        A function used to read in data from a particular run. It must have a positional argument\n",
    "        that will be iterated over, the name of which is given by \"iterate_over\"\n",
    "    member_list : list\n",
    "        List of member numbers to read\n",
    "    args : tuple\n",
    "        List of positional arguments to pass to f\n",
    "    iterate_over : str\n",
    "        The name of the positional argument in args to iterate over\n",
    "    process_parallel : bool\n",
    "        Whether to process the runs in parallel or not (using joblib)\n",
    "    n_jobs : int\n",
    "        Number of jobs to run in parallel\n",
    "    verbose : int\n",
    "        verbosity level passed on to joblib.Parallel\n",
    "    kwargs : dict\n",
    "        List of keyword arguments to pass to f\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of updated run dictionaries\n",
    "\n",
    "    \"\"\"\n",
    "    # The following avoids this gotcha:\n",
    "    # https://pythonconquerstheuniverse.wordpress.com/2012/02/15/mutable-default-arguments/\n",
    "    if f_args is None:\n",
    "        f_args = ()\n",
    "    if f_kwargs is None:\n",
    "        f_kwargs = {}\n",
    "    # Get a list of the function argument names, and find the one that we want to iterate over\n",
    "    f_argnames = inspect.getargspec(f).args\n",
    "    try:\n",
    "        arg_to_iterate = f_argnames.index(iterate_over)\n",
    "    except ValueError:\n",
    "        print(\"{} not found in the function argument list! Stopping!\".format(iterate_over))\n",
    "        return\n",
    "\n",
    "    # Define a wrapper function to replace the value of the iterated argument in f with a new one\n",
    "    # Not sure if this is the most elegant way to do this, but it works and lets us use the\n",
    "    # joblib.Parallel function with list comprehension for the \"val\" argument\n",
    "    def g(f, val):\n",
    "        new_args = tuple(val if i == arg_to_iterate else f_args[i] for i in range(len(f_args)))\n",
    "        return f(*new_args, **f_kwargs)\n",
    "\n",
    "    if process_parallel:\n",
    "        runs = Parallel(n_jobs=n_jobs, verbose=verbose)(delayed(g)(f, val) for val in member_list)\n",
    "    else:\n",
    "        runs = []\n",
    "        for val in member_list:\n",
    "            print(\"Reading files for member {:d}\".format(val))\n",
    "            runs.append(g(f, val))\n",
    "    return runs\n",
    "\n",
    "\n",
    "def read_member_data(basedir, expname, member, cycle, fileformat, time_range, varnames, filetype='history', \n",
    "                     ibgn=None, iend=None, jbgn=None, jend=None, klvls=[1], nproc_x=1, nproc_y=1, dump_nc_output=True,\n",
    "                     ncdir=None, datetime_range=None, x=None, y=None, mid_diameters=None):\n",
    "    print(\"Loading member #{:d}\".format(member))\n",
    "    # Get member run prefix\n",
    "    member_dir, member_prefix = sim.get_ARPS_member_dir_and_prefix(member, cycle)\n",
    "    member_absdir = os.path.join(basedir, expname, member_dir)\n",
    "    vardict_list = []\n",
    "    # all_files_exist_list = []\n",
    "    for time in time_range:\n",
    "        print(\"Loading time \", time) \n",
    "        # Get member file path\n",
    "        filepath = arps_read.get_file_path(member_absdir, member_prefix, fileformat, time=time, \n",
    "                                           filetype='history')\n",
    "#         all_files_exist = True\n",
    "#         for ip in range(1, nproc_x + 1):\n",
    "#             for jp in range(1, nproc_y + 1):\n",
    "#                 filenamepatch = filepath + \"_%03d\" % ip + \"%03d\" % jp\n",
    "#                 if not os.path.exists(filenamepatch):\n",
    "#                     print(\"File {} does not exist!\".format(filenamepatch))\n",
    "#                     all_files_exist = False\n",
    "#         all_files_exist_list.append(all_files_exist)\n",
    "        # Get variables from file\n",
    "        vardict = arps_read.read_hdfvars(filepath, varnames, ibgn=ibgn, jbgn=jbgn, iend=iend, jend=jend,\n",
    "                                         klvls=klvls, nproc_x=nproc_x, nproc_y=nproc_y)\n",
    "        vardict_list.append(vardict)\n",
    "    \n",
    "    if dump_nc_output:\n",
    "        dump_nc(ncdir=ncdir, vardict_list=vardict_list, member_prefix=member_prefix, time=datetime_range, \n",
    "                x=x, y=y, mid_diameters=mid_diameters, ibgn=ibgn, iend=iend, jbgn=jbgn, jend=jend)\n",
    "    \n",
    "    return vardict_list  #, all_files_exist_list\n",
    "\n",
    "\n",
    "def dump_nc(ncdir=None, vardict_list=None, member_prefix=None, time=None, x=None, y=None, mid_diameters=None, \n",
    "            ibgn=None, iend=None, jbgn=None, jend=None):\n",
    "    \n",
    "    coord_dict = {'time': time,\n",
    "                  'yc': ('yc', y),\n",
    "                  'xc': ('xc', x)}\n",
    "    # First, create a dict of lists out of the above list of dicts\n",
    "    vardict_combined = CRMutils.make_dict_of_lists(vardict_list)\n",
    "    # Set things up for creating the xr Dataset\n",
    "    for varname, var in vardict_combined.items():\n",
    "        var_arr = np.array(var).T.squeeze()\n",
    "        var_arr = np.rollaxis(var_arr, 2, 0)\n",
    "        # Trim variables down to just the patch we want to work with\n",
    "        var_arr_patch = var_arr[:, jbgn:jend+1, ibgn:iend+1]\n",
    "        vardict_combined[varname] = (['time', 'yc', 'xc'], var_arr_patch)\n",
    "\n",
    "    # Create an xarray Dataset out of the variable dictionary\n",
    "    var_ds = xr.Dataset(vardict_combined, coords=coord_dict)\n",
    "\n",
    "    # Compute raw model DSD paramters and add them to the model Dataset\n",
    "    rhor = 1000.\n",
    "    cr = np.pi / 6. * rhor\n",
    "    var_ds['rho'] = thermo.calrho(var_ds['p'], var_ds['pt'], var_ds['qv'])\n",
    "\n",
    "    # Shape parameter\n",
    "    var_ds['alphar'] = dsd.solve_alpha(var_ds['rho'], cr, var_ds['qr'], var_ds['nr'], var_ds['zr'])\n",
    "    # var_ds['alphar'] = var_ds['alphar'].interpolate_na()\n",
    "    # Intercept parameter\n",
    "    var_ds['N0r'] = dsd.calc_N0_gamma(var_ds['rho'], var_ds['qr'], var_ds['nr'], cr, var_ds['alphar'])\n",
    "    # Slope parameter\n",
    "    var_ds['lamdar'] = dsd.calc_lamda_gamma(var_ds['rho'], var_ds['qr'], var_ds['nr'], cr, var_ds['alphar'])\n",
    "\n",
    "    # Try computing ND and logND here\n",
    "\n",
    "    # Broadcast DSD parameter DataArrays to get everyone on the same dimensional page\n",
    "    mid_diameters, N0r_model_da, lamdar_model_da, alphar_model_da = \\\n",
    "        xr.broadcast(mid_diameters, var_ds['N0r'], var_ds['lamdar'] , var_ds['alphar'])\n",
    "\n",
    "    # Transpose these DataArrays to get time as the first dimension\n",
    "    mid_diameters = mid_diameters.transpose('time', 'diameter_bin', 'yc', 'xc')\n",
    "    N0r_model_da = N0r_model_da.transpose('time', 'diameter_bin', 'yc', 'xc')\n",
    "    lamdar_model_da = lamdar_model_da.transpose('time', 'diameter_bin', 'yc', 'xc')\n",
    "    alphar_model_da = alphar_model_da.transpose('time', 'diameter_bin', 'yc', 'xc')\n",
    "\n",
    "    ND_model = dsd.calc_binned_DSD_from_params(N0r_model_da, lamdar_model_da, alphar_model_da, mid_diameters)\n",
    "    ND_model = ND_model.fillna(0.0)\n",
    "    logND_model = np.log10(ND_model)\n",
    "    logND_model = logND_model.where(logND_model > -np.inf)\n",
    "    #logND_model = logND_model.fillna(0.0)\n",
    "    #logND_model = logND_model.where(logND_model > -1.0)\n",
    "\n",
    "    var_ds['ND'] = ND_model\n",
    "    var_ds['logND'] = logND_model\n",
    "    print(var_ds)\n",
    "    # Save Dataset to nc file\n",
    "    filename = \"{}_fields.nc\".format(member_prefix)\n",
    "    filepath = os.path.join(ncdir, filename)\n",
    "    var_ds.to_netcdf(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T20:20:43.800099Z",
     "start_time": "2019-09-09T20:20:43.055312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197 254 226 293\n",
      "dict_keys(['runname', 'nens', 'fileformat', 'microphys', 'model_dt', 'model_dt_mean', 'basedirname', 'timestamp_model_init', 'timestamp_model_start', 'timestamp_model_stop'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dawson29/.conda/envs/cent7/5.1.0-py27/pyPIPS/lib/python3.7/site-packages/ipykernel_launcher.py:44: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "[Parallel(n_jobs=12)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done   1 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=12)]: Done   8 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=12)]: Done  17 out of  36 | elapsed:  9.2min remaining: 10.3min\n",
      "[Parallel(n_jobs=12)]: Done  21 out of  36 | elapsed:  9.3min remaining:  6.6min\n",
      "[Parallel(n_jobs=12)]: Done  25 out of  36 | elapsed: 12.4min remaining:  5.5min\n",
      "[Parallel(n_jobs=12)]: Done  29 out of  36 | elapsed: 13.8min remaining:  3.3min\n",
      "[Parallel(n_jobs=12)]: Done  33 out of  36 | elapsed: 13.9min remaining:  1.3min\n",
      "[Parallel(n_jobs=12)]: Done  36 out of  36 | elapsed: 14.0min finished\n"
     ]
    }
   ],
   "source": [
    "print(ibgn, iend, jbgn, jend)\n",
    "print(init_model_dict.keys())\n",
    "varnames = ['p', 'pt', 'qv', 'u', 'v', 'qr', 'nr', 'zr']\n",
    "member_list = range(1, num_members+1)\n",
    "klvls=[1]\n",
    "ncdir = '/depot/dawson29/data/Projects/vortexse_enkf_dsd_study/data/nc'\n",
    "xc_patch = xc[ibgn:iend+1]\n",
    "yc_patch = yc[jbgn:jend+1]\n",
    "mid_diameters_da = vd_matrix_da_dict['PIPS1A']['diameter']\n",
    "\n",
    "\n",
    "# Pack the args and kwargs for the call to the parallel reader wrapper function\n",
    "args = (basedir, expname, member, cycle, fileformat, trange, varnames)\n",
    "kwargs = {'filetype': filetype, 'ibgn': ibgn, 'iend': iend, 'jbgn': jbgn, 'jend': jend, 'klvls': klvls,\n",
    "          'nproc_x': nproc_x, 'nproc_y': nproc_y, 'ncdir': ncdir, 'datetime_range': datetime_range, \n",
    "          'x': xc_patch, 'y': yc_patch, 'mid_diameters': mid_diameters_da}\n",
    "\n",
    "# Set up the parallel read of all the model members\n",
    "runs_list = read_ensemble(read_member_data, member_list, f_args=args, f_kwargs=kwargs, iterate_over='member',\n",
    "                          process_parallel=True, n_jobs=12, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "pyPIPS",
   "language": "python",
   "name": "pypips"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
