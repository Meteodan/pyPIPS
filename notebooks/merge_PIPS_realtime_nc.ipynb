{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T21:18:48.553425Z",
     "start_time": "2019-09-03T21:18:48.478698Z"
    }
   },
   "outputs": [],
   "source": [
    "#This notebook is for plotting of PIPS data from the real-time script's netCDF file dumps and merging with netCDF\n",
    "#files produced from the onboard card data if necessary\n",
    "%matplotlib notebook\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from pyPIPS import thermolib as thermo\n",
    "from pyPIPS import timemodule as tm\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.dates as dates\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid, make_axes_locatable, host_subplot\n",
    "import pyPIPS.plotmodule as pm\n",
    "from pyPIPS.PIPS import avg_diameter, avg_fall_bins, max_diameter, \\\n",
    "    min_diameter, min_fall_bins, diameter_edges, fall_bins_edges\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T21:18:49.917577Z",
     "start_time": "2019-09-03T21:18:49.821037Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function definitions. \n",
    "\n",
    "def get_files(file_list, starttime, endtime, ftype='onesec'):\n",
    "    ''' this seems like a janky way to do it, but is actually 3x faster than\n",
    "        making a loop of files.'''\n",
    "\n",
    "    len_prefix = 8 + len(ftype)\n",
    "\n",
    "    day_str = [(starttime + timedelta(days=i)).strftime(\"%Y%m%d\")\n",
    "               for i in range((endtime - starttime).days + 1)]\n",
    "\n",
    "    # find all PIPS files with days between starttime and endtime\n",
    "    file_list = [file_name for file_name in file_list if any(day in file_name for day in day_str)]\n",
    "    # file_list = [f for subf in file_list for f in subf]  # flatten list in case of multiple days\n",
    "\n",
    "    if file_list:\n",
    "        # sort files by date, then find nearest indices for all the dates, and loop over that\n",
    "        sorted_files = sorted(file_list,\n",
    "                              key=lambda f: datetime.strptime(f[len_prefix:len_prefix + 14],\n",
    "                                                              '%Y%m%d%H%M%S'))\n",
    "        starttimes = [datetime.strptime(f[len_prefix:len_prefix + 14], '%Y%m%d%H%M%S')\n",
    "                      for f in sorted_files]\n",
    "        endtimes = [datetime.strptime(f[len_prefix + 15:len_prefix + 29], '%Y%m%d%H%M%S')\n",
    "                    for f in sorted_files]\n",
    "        _, idx1 = min((abs(val - starttime), idx) for (idx, val) in enumerate(starttimes))\n",
    "        _, idx2 = min((abs(val - endtime), idx) for (idx, val) in enumerate(endtimes))\n",
    "        file_list = sorted_files[idx1:idx2 + 1]\n",
    "\n",
    "    return file_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T21:18:50.671417Z",
     "start_time": "2019-09-03T21:18:50.612780Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up directories reading the data and plotting\n",
    "# base_output_dir = '/Users/dawson29/sshfs_mounts/depot/data/Projects/TriPIPS/webdata/'\n",
    "deployment_name = 'IOP2_030323'\n",
    "\n",
    "base_realtime_input_dir = '/Users/dawson29/Dropbox/Projects/PERiLS/PIPS_data/2023_realtime'\n",
    "realtime_netcdf_input_dir = os.path.join(base_realtime_input_dir, deployment_name)\n",
    "\n",
    "base_card_input_dir = '/Users/dawson29/Dropbox/PIPS_data/2023/'\n",
    "card_netcdf_input_dir = os.path.join(base_card_input_dir, deployment_name, 'netcdf')\n",
    "\n",
    "plot_output_dir = os.path.join(base_realtime_input_dir, 'plots')\n",
    "\n",
    "if not os.path.exists(plot_output_dir):\n",
    "    os.makedirs(plot_output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the PIPS name, dates and times to read and plot\n",
    "\n",
    "PIPS_name = 'PIPS1A'\n",
    "\n",
    "# PIPS1A\n",
    "starttime_stamp = '20230303081700'\n",
    "endtime_stamp = '20230303115100'\n",
    "\n",
    "# PIPS2A\n",
    "# starttime_stamp = '20230303084100'\n",
    "# endtime_stamp = '20230303111800'\n",
    "\n",
    "\n",
    "starttime_dt = datetime.strptime(starttime_stamp, tm.timefmt3)\n",
    "endtime_dt = datetime.strptime(endtime_stamp, tm.timefmt3)\n",
    "\n",
    "file_path_list_onePIPS = glob.glob(realtime_netcdf_input_dir + f'/*{PIPS_name}*nc')\n",
    "file_list_onePIPS = [os.path.basename(file_path) for file_path in file_path_list_onePIPS]\n",
    "\n",
    "file_list_onesec = [file_name for file_name in file_list_onePIPS if 'onesec' in file_name]\n",
    "file_list_onesec = [file_name for file_name in file_list_onesec if 'current' not in file_name]\n",
    "file_list_onesec = get_files(file_list_onesec, starttime_dt, endtime_dt)\n",
    "file_path_list_onesec = [os.path.join(realtime_netcdf_input_dir, file_name) for file_name in file_list_onesec]\n",
    "\n",
    "\n",
    "file_list_ND = [file_name for file_name in file_list_onePIPS if 'ND' in file_name]\n",
    "file_list_ND = [file_name for file_name in file_list_ND if 'current' not in file_name]\n",
    "file_list_ND = get_files(file_list_ND, starttime_dt, endtime_dt, ftype='ND')\n",
    "file_path_list_ND = [os.path.join(realtime_netcdf_input_dir, file_name) for file_name in file_list_ND]\n",
    "\n",
    "file_list_spectrum = [file_name for file_name in file_list_onePIPS if 'spectrum' in file_name]\n",
    "file_list_spectrum = [file_name for file_name in file_list_spectrum if 'current' not in file_name]\n",
    "file_list_spectrum = get_files(file_list_spectrum, starttime_dt, endtime_dt, ftype='spectrum')\n",
    "file_path_list_spectrum = [os.path.join(realtime_netcdf_input_dir, file_name) for file_name in file_list_spectrum]\n",
    "\n",
    "file_list_telegram = [file_name for file_name in file_list_onePIPS if 'telegram' in file_name]\n",
    "file_list_telegram = [file_name for file_name in file_list_telegram if 'current' not in file_name]\n",
    "file_list_telegram = get_files(file_list_telegram, starttime_dt, endtime_dt, ftype='telegram')\n",
    "file_path_list_telegram = [os.path.join(realtime_netcdf_input_dir, file_name) for file_name in file_list_telegram]\n",
    "\n",
    "# datetime_stamp = '20220505210000'\n",
    "# ND_file = 'ND_{}.nc'.format(datetime_stamp)\n",
    "# ND_path = os.path.join(netcdf_input_dir, ND_file)\n",
    "# ND_ds = xr.load_dataset(ND_path)\n",
    "\n",
    "# conv_file = 'onesec_{}.nc'.format(datetime_stamp)\n",
    "# conv_path = os.path.join(netcdf_input_dir, conv_file)\n",
    "# conv_ds = xr.load_dataset(conv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now read in netCDF files\n",
    "onesec_ds = xr.open_mfdataset(file_path_list_onesec, combine='nested', concat_dim='logger_datetime')\n",
    "onesec_ds = onesec_ds.rename({'logger_datetime': 'time'})\n",
    "onesec_ds = onesec_ds.drop_duplicates('time')\n",
    "ND_ds = xr.open_mfdataset(file_path_list_ND, combine='nested', concat_dim='time')\n",
    "ND_ds = ND_ds.drop_duplicates('time')\n",
    "ND_ds = ND_ds.rename_dims({'diameter': 'diameter_bin'})\n",
    "spectrum_ds = xr.open_mfdataset(file_path_list_spectrum, combine='nested', concat_dim='time')\n",
    "spectrum_ds = spectrum_ds.drop_duplicates('time')\n",
    "spectrum_ds = spectrum_ds.rename_dims({'diameter': 'diameter_bin', 'velocity': 'velocity_bin'})\n",
    "telegram_ds = xr.open_mfdataset(file_path_list_telegram, combine='nested', concat_dim='index')\n",
    "telegram_ds = telegram_ds.rename({'index': 'time'})\n",
    "telegram_ds = telegram_ds.drop_duplicates('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read netCDF files from card\n",
    "parsivel_combined_card_filename = f'parsivel_combined_{deployment_name}_{PIPS_name}_10s.nc'\n",
    "parsivel_combined_card_path = os.path.join(card_netcdf_input_dir, parsivel_combined_card_filename)\n",
    "\n",
    "onesec_card_filename = f'conventional_raw_{deployment_name}_{PIPS_name}.nc'\n",
    "onesec_card_path = os.path.join(card_netcdf_input_dir, onesec_card_filename)\n",
    "\n",
    "parsivel_combined_card_ds = xr.open_dataset(parsivel_combined_card_path)\n",
    "onesec_card_ds = xr.open_dataset(onesec_card_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct times from from the real-time files to match the GPS times. This is already done for the times\n",
    "# from the card-derived netCDF files\n",
    "\n",
    "# Get first good GPS time in file\n",
    "\n",
    "first_good_GPS_time = onesec_ds.where(onesec_ds['GPS_status'] == 'A', drop=True).isel(time=0)\n",
    "\n",
    "logger_datetime = first_good_GPS_time['time'].values\n",
    "logger_datetime = pd.to_datetime(logger_datetime).to_pydatetime()\n",
    "\n",
    "GPS_date = str(first_good_GPS_time['GPS_date'].values)\n",
    "GPS_time = str(first_good_GPS_time['GPS_time'].values)\n",
    "\n",
    "# Next, construct datetime object from GPS info\n",
    "# Construct datetime object\n",
    "gyear = int('20' + GPS_date[4:])\n",
    "gmonth = int(GPS_date[2:4])\n",
    "gday = int(GPS_date[:2])\n",
    "ghour = int(GPS_time[:2])\n",
    "gmin = int(GPS_time[2:4])\n",
    "gsec = int(GPS_time[4:6])\n",
    "\n",
    "GPS_datetime = datetime(gyear, gmonth, gday, ghour, gmin, gsec)\n",
    "GPS_offset = GPS_datetime - logger_datetime\n",
    "print('GPS time: {}, Logger time: {}'.format(GPS_datetime.ctime(),\n",
    "                                             logger_datetime.ctime()))\n",
    "print('GPS Offset: {}'.format(str(GPS_offset)))\n",
    "\n",
    "# print(onesec_card_ds.where(onesec_card_ds['GPS_status'] == 'A', drop=True).isel(time=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_times = pd.to_datetime(onesec_ds['time']).to_pydatetime()\n",
    "new_times = old_times + GPS_offset\n",
    "onesec_ds = onesec_ds.assign_coords({'time': new_times})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_ND_times = pd.to_datetime(ND_ds['time']).to_pydatetime()\n",
    "new_times = old_ND_times + GPS_offset\n",
    "ND_ds = ND_ds.assign_coords({'time': new_times})\n",
    "\n",
    "old_spectrum_times = pd.to_datetime(spectrum_ds['time']).to_pydatetime()\n",
    "new_times = old_spectrum_times + GPS_offset\n",
    "spectrum_ds = spectrum_ds.assign_coords({'time': new_times})\n",
    "\n",
    "old_telegram_times = pd.to_datetime(telegram_ds['time']).to_pydatetime()\n",
    "new_times = old_telegram_times + GPS_offset\n",
    "telegram_ds = telegram_ds.assign_coords({'time': new_times})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, figure out what times are missing from the data for each source\n",
    "\n",
    "# First find earliest start and latest end times\n",
    "tensec_card_starttime = parsivel_combined_card_ds['time'][0].values\n",
    "tensec_card_endtime = parsivel_combined_card_ds['time'][-1].values\n",
    "spectrum_starttime = spectrum_ds['time'][0].values\n",
    "spectrum_endtime = spectrum_ds['time'][-1].values\n",
    "telegram_starttime = telegram_ds['time'][0].values\n",
    "telegram_endtime = telegram_ds['time'][-1].values\n",
    "ND_starttime = ND_ds['time'][0].values\n",
    "ND_endtime = ND_ds['time'][-1].values\n",
    "\n",
    "starttime_tensec = min(tensec_card_starttime, spectrum_starttime, telegram_starttime, ND_starttime)\n",
    "endtime_tensec = max(tensec_card_endtime, spectrum_endtime, telegram_endtime, ND_endtime)\n",
    "\n",
    "print(starttime_tensec, endtime_tensec)\n",
    "\n",
    "# Then, create a new index of all times at 10-s intervals between the two\n",
    "\n",
    "all_tensec_times = xr.date_range(starttime_tensec, endtime_tensec, freq='10S')\n",
    "print(all_tensec_times)\n",
    "print(all_tensec_times[0], all_tensec_times[-1])\n",
    "\n",
    "print(ND_ds['time'])\n",
    "print(spectrum_ds['time'])\n",
    "print(parsivel_combined_card_ds['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now reindex all datasets with the full set of times\n",
    "\n",
    "ND_ds_full = ND_ds.reindex({'time': all_tensec_times})\n",
    "print(ND_ds_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T21:18:51.314834Z",
     "start_time": "2019-09-03T21:18:51.244790Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up dictionaries to control plotting parameters\n",
    "\n",
    "dateformat = '%H:%M'\n",
    "\n",
    "# Temperature and dewpoint\n",
    "temp_dewp_ax_params = {\n",
    "    'majorxlocator': dates.MinuteLocator(byminute=[0, 15, 30, 45], interval=1), \n",
    "    'majorxformatter': dates.DateFormatter(dateformat),\n",
    "    'minorxlocator': dates.MinuteLocator(byminute=[0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55], interval=1),\n",
    "    'axeslimits': [None, (-5., 35.)],\n",
    "    'axeslabels': ['Time (H:M) UTC', r'Temperature ($^{\\circ}$C)']\n",
    "}\n",
    "\n",
    "# Wind speed and direction\n",
    "windspd_ax_params = {\n",
    "    'majorxlocator': dates.MinuteLocator(byminute=[0, 15, 30, 45], interval=1), \n",
    "    'majorxformatter': dates.DateFormatter(dateformat),\n",
    "    'minorxlocator': dates.MinuteLocator(byminute=[0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55], interval=1),\n",
    "    'axeslimits': [None, [0.0, 25.0]],\n",
    "    'axeslabels': ['Time (H:M) UTC', r'wind speed (m s$^{-1}$)']\n",
    "}\n",
    "\n",
    "winddir_ax_params = {\n",
    "    'majorylocator': ticker.MultipleLocator(45.),\n",
    "    'axeslimits': [None, [0.0, 360.0]],\n",
    "    'axeslabels': [None, r'Wind direction ($^{\\circ}$C)']\n",
    "}\n",
    "\n",
    "pressure_ax_params = {\n",
    "    'majorylocator': ticker.MultipleLocator(5.),\n",
    "    'axeslimits': [None, [940., 980.]],\n",
    "    'axeslabels': [None, r'Pressure (hPa)']\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Number concentration\n",
    "log_ND_params = {\n",
    "    'type': 'pcolor', \n",
    "    'vlimits': [-1.0, 3.0],\n",
    "    'clabel': r'log[N ($m^{-3} mm^{-1}$)]'\n",
    "}\n",
    "\n",
    "log_ND_ax_params = {\n",
    "    'majorxlocator': dates.MinuteLocator(byminute=[0, 15, 30, 45], interval=1), \n",
    "    'majorxformatter': dates.DateFormatter(dateformat),\n",
    "    'minorxlocator': dates.MinuteLocator(byminute=[0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55], interval=1),\n",
    "    'axeslimits': [None, [0.0, 9.0]],\n",
    "    'majorylocator': ticker.MultipleLocator(base=0.25),\n",
    "    'axeslabels': [None, 'D (mm)']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Quick DSD meteogram\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plottimes_tmp = ND_ds_full['time'].to_index().to_pydatetime()\n",
    "# Prepend an additional at the beginning of the array so that pcolor sees this as the\n",
    "# edges of the DSD intervals.\n",
    "plottimes = np.insert(plottimes_tmp, 0, plottimes_tmp[0] - timedelta(seconds=10))\n",
    "plottimes = [plottimes]\n",
    "\n",
    "ND_da = ND_ds_full['ND']\n",
    "\n",
    "ND_arr = ND_da.values.T\n",
    "logND_arr = np.ma.log10(ND_arr)\n",
    "fields_to_plot = [logND_arr]\n",
    "field_parameters = [log_ND_params]\n",
    "ax = pm.plotmeteogram(ax, plottimes, fields_to_plot, field_parameters,\n",
    "                      yvals=[diameter_edges] * len(fields_to_plot))\n",
    "ax, = pm.set_meteogram_axes([ax], [log_ND_ax_params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick DSD meteogram for card data\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plottimes_tmp = parsivel_combined_card_ds['time'].to_index().to_pydatetime()\n",
    "# Prepend an additional at the beginning of the array so that pcolor sees this as the\n",
    "# edges of the DSD intervals.\n",
    "plottimes = np.insert(plottimes_tmp, 0, plottimes_tmp[0] - timedelta(seconds=10))\n",
    "plottimes = [plottimes]\n",
    "\n",
    "ND_da_card = parsivel_combined_card_ds['ND']\n",
    "\n",
    "ND_arr = ND_da_card.values.T\n",
    "logND_arr = np.ma.log10(ND_arr)\n",
    "fields_to_plot = [logND_arr]\n",
    "field_parameters = [log_ND_params]\n",
    "ax = pm.plotmeteogram(ax, plottimes, fields_to_plot, field_parameters,\n",
    "                      yvals=[diameter_edges] * len(fields_to_plot))\n",
    "ax, = pm.set_meteogram_axes([ax], [log_ND_ax_params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to merge the data to fill in gaps\n",
    "ND_da_new = ND_da.combine_first(ND_da_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ND_da_new)\n",
    "print(ND_da)\n",
    "print(ND_da_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ND_da_new.equals(ND_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ND_da_new.drop_duplicates('time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ND_da_new.sel(time=slice('2023-03-03 T10:05', '2023-03-03 T10:20')).coords['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick DSD meteogram for merged data\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plottimes_tmp = ND_da_new.coords['time'].to_index().to_pydatetime()\n",
    "# Prepend an additional at the beginning of the array so that pcolor sees this as the\n",
    "# edges of the DSD intervals.\n",
    "plottimes = np.insert(plottimes_tmp, 0, plottimes_tmp[0] - timedelta(seconds=10))\n",
    "plottimes = [plottimes]\n",
    "\n",
    "ND_arr = ND_da_new.values.T\n",
    "logND_arr = np.ma.log10(ND_arr)\n",
    "fields_to_plot = [logND_arr]\n",
    "field_parameters = [log_ND_params]\n",
    "ax = pm.plotmeteogram(ax, plottimes, fields_to_plot, field_parameters,\n",
    "                      yvals=[diameter_edges] * len(fields_to_plot))\n",
    "ax, = pm.set_meteogram_axes([ax], [log_ND_ax_params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick DSD plot\n",
    "fig_dsd, ax_dsd = plt.subplots(figsize=(10, 10))\n",
    "time_to_plot = '20220505213500'\n",
    "datetime_to_plot = datetime.strptime(time_to_plot, tm.timefmt3)\n",
    "datetime_minus_5min = datetime_to_plot - timedelta(minutes=5)\n",
    "ND_avg_da = ND_da.sel(time=slice(datetime_minus_5min, datetime_to_plot)).mean(dim='time')\n",
    "\n",
    "ND_avg = ND_avg_da.values\n",
    "ax_dsd.set_title('DSD for time period {0} to {1}'.format(\n",
    "    datetime_minus_5min.strftime(tm.timefmt2),\n",
    "    datetime_to_plot.strftime(tm.timefmt2)))\n",
    "ax_dsd.bar(min_diameter, ND_avg * 1000.0, max_diameter - min_diameter, 10.**2., align='edge',\n",
    "           log=True, color='tan', edgecolor='k')\n",
    "ax_dsd.set_xlim(0.0, 3.0)\n",
    "ax_dsd.set_ylim(10.**2., 10.**6.5)\n",
    "ax_dsd.set_xlabel('D (mm)')\n",
    "ax_dsd.set_ylabel(r'N(D) $(m^{-4})$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_t_td, ax_t_td = plt.subplots(figsize=(10, 6))\n",
    "plottimes_onesec = [conv_ds['index'].to_index().to_pydatetime()]\n",
    "# Temperature and Dewpoint\n",
    "Tmin = np.nanmin(conv_ds['Dewpoint'].values)\n",
    "Tmax = np.nanmax(conv_ds['SlowTemp'].values)\n",
    "fields_to_plot_onesec = [conv_ds['SlowTemp'].values, conv_ds['Dewpoint'].values]\n",
    "temp_params = pm.temp_params.copy()\n",
    "dewpoint_params = pm.dewpoint_params.copy()\n",
    "temp_params['plotmin'] = Tmin - 5.0\n",
    "dewpoint_params['plotmin'] = Tmin - 5.0\n",
    "field_parameters_onesec = [temp_params, dewpoint_params]\n",
    "ax_t_td = pm.plotmeteogram(\n",
    "    ax_t_td,\n",
    "    plottimes_onesec,\n",
    "    fields_to_plot_onesec,\n",
    "    field_parameters_onesec)\n",
    "temp_dewp_ax_params['axeslimits'] = [[plottimes_onesec[0][0], plottimes_onesec[0][-1]],\n",
    "                                     [Tmin - 5.0, Tmax + 5.0]]\n",
    "ax_t_td, = pm.set_meteogram_axes([ax_t_td], [temp_dewp_ax_params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-03T22:58:45.319865Z",
     "start_time": "2019-09-03T21:19:34.398406Z"
    }
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#for i in range(numiter):\n",
    "while True:\n",
    "    ax.clear()\n",
    "    ax_vd.clear()\n",
    "    ax_t_td.clear()\n",
    "    ax_windspd.clear()\n",
    "    ax_winddir.clear()\n",
    "    ax_pressure.clear()\n",
    "    onesec_new_df = scrape_tripips_onesec_data(url_onesec, numrecords=numrecords_append_onesec)\n",
    "    onesec_new_df['Dewpoint'] = thermo.calTdfromRH(onesec_new_df['Pressure'] * 100., onesec_new_df['SlowTemp'] + 273.15, \n",
    "                                     onesec_new_df['RH'] / 100.) - 273.15\n",
    "    # Append new data onto onesec_df\n",
    "    onesec_df = onesec_df.append(onesec_new_df)\n",
    "    # Drop duplicate timestamps\n",
    "    onesec_df = onesec_df[~onesec_df.index.duplicated(keep='first')]\n",
    "    # Drop records older than desired interval\n",
    "    last_timestamp_onesec = onesec_df.index[-1]\n",
    "    oldest_timestamp_onesec = last_timestamp_onesec-keep_data_for_ts\n",
    "    onesec_df = onesec_df.loc[oldest_timestamp_onesec:]\n",
    "    # Dump onesec dataframe to netCDF file (via xarray)\n",
    "    netcdf_filename = 'onesec_{}_{}.nc'.format(oldest_timestamp_onesec.strftime('%Y%m%d%H%M%S'), \n",
    "                                               last_timestamp_onesec.strftime('%Y%m%d%H%M%S' ))\n",
    "    netcdf_path = os.path.join(netcdf_output_dir, netcdf_filename)\n",
    "    onesec_df.to_xarray().to_netcdf(netcdf_path)\n",
    "    plottimes_onesec = [onesec_df.index.to_pydatetime()]\n",
    "    # Temperature and Dewpoint\n",
    "    fields_to_plot_onesec = [onesec_df['SlowTemp'].values, onesec_df['Dewpoint'].values]\n",
    "    field_parameters_onesec = [pm.temp_params, pm.dewpoint_params]\n",
    "    ax_t_td = pm.plotmeteogram(ax_t_td, plottimes_onesec, fields_to_plot_onesec, field_parameters_onesec)\n",
    "    temp_dewp_ax_params['axeslimits'][0] = (plottimes_onesec[0][0], plottimes_onesec[0][-1]) \n",
    "    ax_t_td, = pm.set_meteogram_axes([ax_t_td], [temp_dewp_ax_params])\n",
    "    # Wind speed and direction\n",
    "    ax_windspd = pm.plotmeteogram(ax_windspd, plottimes_onesec, [onesec_df['WS_ms'].values], [pm.windspeed_params])\n",
    "    ax_winddir = pm.plotmeteogram(ax_winddir, plottimes_onesec, [onesec_df['WindDir'].values], [pm.winddir_params])\n",
    "    windspd_ax_params['axeslimits'][0] = (plottimes_onesec[0][0], plottimes_onesec[0][-1])\n",
    "    winddir_ax_params['axeslimits'][0] = (plottimes_onesec[0][0], plottimes_onesec[0][-1])\n",
    "    ax_windspd, ax_winddir = pm.set_meteogram_axes([ax_windspd, ax_winddir], [windspd_ax_params, winddir_ax_params])\n",
    "    # Pressure\n",
    "    pmin = np.nanmin(onesec_df['Pressure'].values)\n",
    "    pmax = np.nanmax(onesec_df['Pressure'].values)\n",
    "    pressure_ax_params['axeslimits'] = [None, [pmin - 2.5, pmax + 2.5]]\n",
    "    fields_to_plot_press = [onesec_df['Pressure'].values]\n",
    "    field_parameters_press = [pm.pressure_params]\n",
    "    ax_pressure = pm.plotmeteogram(ax_pressure, plottimes_onesec, fields_to_plot_press, field_parameters_press)\n",
    "    ax_pressure, = pm.set_meteogram_axes([ax_pressure], [pressure_ax_params])\n",
    "    \n",
    "\n",
    "    # DSD plots\n",
    "    telegram_new_df, spectrum_new_da = scrape_tripips_tensec_data(url_tensec, numrecords=numrecords_append_tensec)\n",
    "    ND_new_da = calc_ND_da(spectrum_new_da)\n",
    "    # Append new data onto the data array\n",
    "    ND_da = xr.concat([ND_da, ND_new_da], dim='time')\n",
    "    # Drop duplicate timestamps\n",
    "    ND_da = ND_da.groupby('time').first()\n",
    "    # onesec_df = onesec_df[~onesec_df.index.duplicated(keep='first')]\n",
    "    # Drop records older than desired interval\n",
    "    last_timestamp = ND_da['time'].to_index()[-1]\n",
    "    spectrum = spectrum_new_da.loc[last_timestamp]\n",
    "    # print(last_timestamp)\n",
    "    oldest_timestamp = last_timestamp-keep_data_for_ts\n",
    "    ND_da = ND_da.loc[oldest_timestamp:]\n",
    "    # Dump ND_da to netcdf file\n",
    "    \n",
    "    netcdf_filename = 'ND_{}_{}.nc'.format(oldest_timestamp.strftime('%Y%m%d%H%M%S'), \n",
    "                                       last_timestamp.strftime('%Y%m%d%H%M%S' ))\n",
    "    netcdf_path = os.path.join(netcdf_output_dir, netcdf_filename)\n",
    "    ND_da.to_dataset(name='ND').to_netcdf(netcdf_path)\n",
    "                                \n",
    "    plottimes = [ND_da['time'].to_index().to_pydatetime()]\n",
    "    ND_arr = ND_da.values.T\n",
    "    logND_arr = np.ma.log10(ND_arr)\n",
    "    fields_to_plot = [logND_arr]\n",
    "    field_parameters = [log_ND_params]\n",
    "    ax = pm.plotmeteogram(ax, plottimes, fields_to_plot, field_parameters, \n",
    "                          yvals=[min_diameter] * len(fields_to_plot))\n",
    "    ax, = pm.set_meteogram_axes([ax], [log_ND_ax_params])\n",
    "    ax_vd.set_title('Fall speed vs. diameter for time {0}'.format(last_timestamp.strftime(tm.timefmt2)))\n",
    "    countsplot = np.ma.masked_where(spectrum.values <= 0, spectrum)\n",
    "    C = ax_vd.pcolor(min_diameter, min_fall_bins, countsplot, vmin=1, vmax=50, edgecolors='w')\n",
    "    divider = make_axes_locatable(ax_vd)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\")\n",
    "    cb = fig_vd.colorbar(C, cax=cax, orientation='vertical')\n",
    "    ax_vd.set_xlim(0.0, 10.0)\n",
    "    ax_vd.xaxis.set_major_locator(ticker.MultipleLocator(2.0))\n",
    "    ax_vd.set_xlabel('diameter (mm)')\n",
    "    ax_vd.set_ylim(0.0, 10.0)\n",
    "    ax_vd.yaxis.set_major_locator(ticker.MultipleLocator(1.0))\n",
    "    ax_vd.set_ylabel('fall speed (m/s)')\n",
    "    \n",
    "    display.display(fig)\n",
    "    display.display(fig_vd)\n",
    "    display.display(fig_t_td)\n",
    "    display.display(fig_wind)\n",
    "    display.display(fig_pressure)\n",
    "    display.clear_output(wait=True)\n",
    "    # fig.canvas.draw()\n",
    "    fig.savefig(os.path.join(image_output_dir, 'logND_current.png'), dpi=300)\n",
    "    fig_vd.savefig(os.path.join(image_output_dir, 'VD_current.png'), dpi=300)\n",
    "    fig_t_td.savefig(os.path.join(image_output_dir, 'T_Td_current.png'), dpi=300)\n",
    "    fig_wind.savefig(os.path.join(image_output_dir, 'wind_current.png'), dpi=300)\n",
    "    fig_pressure.savefig(os.path.join(image_output_dir, 'pressure.png'), dpi=300)\n",
    "    \n",
    "    # plt.pause(0.01)\n",
    "    # Sleep for the desired interval. This may not be perfectly accurate\n",
    "    time.sleep(plot_update_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:pyPIPS]",
   "language": "python",
   "name": "conda-env-pyPIPS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
