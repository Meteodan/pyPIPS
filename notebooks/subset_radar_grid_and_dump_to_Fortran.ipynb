{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T22:55:48.420246Z",
     "start_time": "2021-01-02T22:55:45.665406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## You are using the Python ARM Radar Toolkit (Py-ART), an open source\n",
      "## library for working with weather radar data. Py-ART is partly\n",
      "## supported by the U.S. Department of Energy as part of the Atmospheric\n",
      "## Radiation Measurement (ARM) Climate Research Facility, an Office of\n",
      "## Science user facility.\n",
      "##\n",
      "## If you use this software to prepare a publication, please cite:\n",
      "##\n",
      "##     JJ Helmus and SM Collis, JORS 2016, doi: 10.5334/jors.119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.dates as dates\n",
    "import matplotlib.animation as animation\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid, make_axes_locatable, host_subplot\n",
    "#from mpl_toolkits.basemap import Basemap\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "import os\n",
    "import pyPIPS.utils as utils\n",
    "import pyPIPS.thermolib as thermo\n",
    "import pyPIPS.DSDlib as dsd\n",
    "#import pyPIPS.disdrometer_module as dis\n",
    "import pyPIPS.plotmodule as PIPSplot\n",
    "#import pyPIPS.simulator as sim\n",
    "import pyPIPS.pips_io as pipsio\n",
    "import pyPIPS.PIPS as pips\n",
    "import pyPIPS.parsivel_params as pp\n",
    "import pyPIPS.parsivel_qc as pqc\n",
    "import pyPIPS.radarmodule as radar\n",
    "import pyPIPS.polarimetric as dualpol\n",
    "#from pyCRMtools.modules import plotmodule as plotmod\n",
    "from pyCRMtools.modules import utils as CRMutils\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import glob\n",
    "import numpy.random as random\n",
    "from scipy.stats import gamma, uniform\n",
    "from scipy.special import gamma as gammafunc\n",
    "from scipy import ndimage\n",
    "from scipy import interpolate\n",
    "from metpy.plots import StationPlot\n",
    "import metpy.calc as mpcalc\n",
    "from metpy.calc import wind_components\n",
    "from metpy.cbook import get_test_data\n",
    "from metpy.plots import StationPlot\n",
    "from metpy.plots.wx_symbols import current_weather, sky_cover\n",
    "from metpy.units import units\n",
    "from scipy.signal import medfilt2d\n",
    "import pyart\n",
    "import cartopy.crs as ccrs\n",
    "from io import StringIO\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T22:55:49.001054Z",
     "start_time": "2021-01-02T22:55:48.893869Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function definitions\n",
    "def readESC(sounding_path, interpnan=True, handle=False):\n",
    "    \"\"\"\n",
    "    Reads in a sounding in ESC format from a provided file path or handle (can be a StringIO object or an open\n",
    "    file handle)\n",
    "    \"\"\"\n",
    "    col_names = ['pressure','temperature','dewpoint','u_wind','v_wind','speed','direction','height',\n",
    "                 'Qp_code','Qt_code','Qrh_code','Qu_code','Qv_code']\n",
    "    # First read the file and extract the field widths from the 14th header line\n",
    "    if not handle:\n",
    "        f = open(sounding_path, 'r')\n",
    "    else:\n",
    "        f = sounding_path\n",
    "\n",
    "    # Read in the header and extract some metadata from it\n",
    "    dummy = f.readline()\n",
    "    dummy = f.readline()\n",
    "    header2 = f.readline().strip().split(':')\n",
    "    # Read next header line and extract station id and wmo number from it (if it exists)\n",
    "    staid_wmo_str = header2[1]\n",
    "    if ' / ' in staid_wmo_str:\n",
    "        staid_wmo = staid_wmo_str.strip().split(' / ')\n",
    "        staid = staid_wmo[0][1:4]\n",
    "        wmo = int(staid_wmo[1])\n",
    "    else:\n",
    "        if '. ' in staid_wmo_str:\n",
    "            staid = staid_wmo_str.replace('. ', '').strip()[:4]\n",
    "        else:\n",
    "            staid = staid_wmo_str.strip()[:4]\n",
    "            staid = staid.replace(\" \", \"\")\n",
    "        wmo = 99999\n",
    "    print(staid)\n",
    "    # Read the next header line and extract the location information from it\n",
    "    header3 = f.readline().strip().split(':')\n",
    "    location = header3[1].strip().split(',')\n",
    "    print(location)\n",
    "    lon = np.float(location[2])\n",
    "    lat = np.float(location[3])\n",
    "    elev = np.float(location[4])\n",
    "    # Read the next header line and extract the time information from it\n",
    "    header4 = f.readline().strip()[31:].lstrip()   \n",
    "    sounding_datetime = datetime.strptime(header4, '%Y, %m, %d, %H:%M:%S')\n",
    "    \n",
    "    # Now read and dump the rest of the header\n",
    "    for i in range(9):\n",
    "        f.readline()\n",
    "    \n",
    "    # Except for the last header line, which is used to determine the widths of the fields\n",
    "    line = f.readline().strip().split()\n",
    "    fw = [len(field)+1 for field in line]\n",
    "\n",
    "    # Now read the file into the dataframe, using the extracted field widths\n",
    "    df = pd.read_fwf(f, usecols=[1, 2, 3, 5, 6, 7, 8, 14, 15, 16, 17, 18, 19],\n",
    "                     names=col_names, na_values=['99999.0', '9999.0', '999.0'], widths=fw)\n",
    "    \n",
    "    # For some reason, need to convert all the columns to floating point here, as some get interpreted as strings\n",
    "    # when they shouldn't be...\n",
    "    # print(df['pressure'], df['temperature'])\n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].astype(np.float)\n",
    "    \n",
    "    # Drop rows where height or pressure is NaN. TODO: Can't remember why I have to use reset_index(drop=True). \n",
    "    # Figure this out.\n",
    "    df = df.dropna(subset=('height', 'pressure')).reset_index(drop=True)\n",
    "    # Set the height as the index so we can use it as weights to interpolate other columns across NaN\n",
    "    df = df.set_index('height')\n",
    "    df['height'] = df.index\n",
    "    \n",
    "    if interpnan:\n",
    "        # First convert direction and speed to u, v components\n",
    "        df['u'], df['v'] = mpcalc.wind_components(df['speed'].values*units('m/s'),\n",
    "                                                      df['direction'].values*units.degrees)\n",
    "        # Now interpolate\n",
    "        df = df.interpolate(method='values')\n",
    "        # Finally recompute direction and speed from u, v components\n",
    "        df['speed'] = mpcalc.wind_speed(df['u'].values*units('m/s'), df['v'].values*units('m/s'))\n",
    "        df['direction'] = mpcalc.wind_direction(df['u'].values*units('m/s'), df['v'].values*units('m/s'))\n",
    "    else:\n",
    "        # Drop any rows with all NaN values for T, Td, winds\n",
    "        df = df.dropna(subset=('temperature', 'dewpoint', 'direction', 'speed',\n",
    "                               'u_wind', 'v_wind'), how='all').reset_index(drop=True)\n",
    "    \n",
    "    df = df[(df.Qp_code == 1.0) & (df.Qt_code == 1.0) & (df.Qrh_code == 1.0) & (df.Qu_code == 1.0) & \n",
    "            (df.Qv_code == 1.0)]\n",
    "\n",
    "    nlines = df.count()['pressure']\n",
    "    \n",
    "    if not handle:\n",
    "        f.close()\n",
    "    \n",
    "    snd_metadata = {\n",
    "        'sounding_datetime': sounding_datetime,\n",
    "        'lat': lat,\n",
    "        'lon': lon,\n",
    "        'selev': elev,\n",
    "        'staid': staid,\n",
    "        'wmo': wmo,\n",
    "        'nlevs': nlines,\n",
    "        'staid_long': staid_wmo_str\n",
    "    }\n",
    "    \n",
    "    return snd_metadata, df\n",
    "\n",
    "\n",
    "def readsharppy(path):\n",
    "    \"\"\"Reads in a sounding in sharppy format\"\"\"\n",
    "        ## read in the file\n",
    "    f = open(path, 'r')\n",
    "    lines = f.read()\n",
    "    data = np.array([l.strip() for l in lines.split('\\n')])\n",
    "    f.close()\n",
    "\n",
    "    ## necessary index points\n",
    "    title_idx = np.where( data == '%TITLE%')[0][0]\n",
    "    start_idx = np.where( data == '%RAW%' )[0][0] + 1\n",
    "    finish_idx = np.where( data == '%END%')[0][0]\n",
    "    \n",
    "    ## create the plot title\n",
    "    data_header = data[title_idx + 1].split()\n",
    "    location = data_header[0]\n",
    "    time = data_header[1][:11]\n",
    "\n",
    "    ## put it all together for StringIO\n",
    "    full_data = '\\n'.join(data[start_idx : finish_idx][:])\n",
    "    sound_data = StringIO( full_data )\n",
    "\n",
    "    ## read the data into arrays\n",
    "    p, h, T, Td, wdir, wspd = np.genfromtxt( sound_data, delimiter=',', comments=\"%\", unpack=True )\n",
    "    \n",
    "    col_names = ['pressure','height','temperature','dewpoint','speed','direction']\n",
    "    data_dict = {key:value for (key,value) in zip(col_names,[p,h,T,Td,wspd,wdir])}\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(data_dict)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def roundPartial(value, resolution, decimals=4):\n",
    "    return np.around(np.round(value / resolution) * resolution, decimals=decimals)\n",
    "\n",
    "\n",
    "def rain_Brandes(D):\n",
    "    \"\"\"Given a range of diameters D, compute rain fall speed curve, a quartic polynomial\n",
    "       fit after Brandes et al. (2002).\"\"\"\n",
    "    \n",
    "    D_mm=D*1000. # get it to (mm)\n",
    "    \n",
    "    Vtr = -0.1021 + 4.932*D_mm - 0.9551*D_mm**2. + 0.07934*D_mm**3. - 0.002362*D_mm**4.\n",
    "    \n",
    "    return Vtr\n",
    "\n",
    "\n",
    "def cal_xf_tf(usm, vsm, vt, H, perturb_vt=False, sigma=0.1):\n",
    "    \"\"\"Computes final horizontal position and residence time (relative to starting position) of a raindrop\n",
    "       falling through a horizontally homogeneous layer H with terminal velocity vt and \n",
    "       storm releative mean wind given by (usm, vsm).\"\"\"\n",
    "    \n",
    "    if perturb_vt:\n",
    "        rng = np.random.default_rng()\n",
    "        vt_perts = sigma * rng.standard_normal(vt.size)\n",
    "        vt = vt + vt_perts\n",
    "    \n",
    "    tf = H / vt\n",
    "    xf = tf * usm\n",
    "    yf = tf * vsm\n",
    "    \n",
    "    return xf, yf, tf\n",
    "\n",
    "\n",
    "def mtokm(val,pos):\n",
    "    \"\"\"Convert m to km for formatting axes tick labels\"\"\"\n",
    "    val=val/1000.0\n",
    "    return '%i' % val\n",
    "\n",
    "def interpolate_all(gridded_radar, tinterp_intv, base_field_name='reflectivity_masked'):\n",
    "    # Get list of intervals in seconds between subsequent radar times\n",
    "    tdiffs = gridded_radar['time_seconds'].diff(dim='time')\n",
    "    \n",
    "    # This list will hold all the time-interpolated grids (xarray Datasets). \n",
    "    # Can later be concatenated into a new xarray Dataset containing all times\n",
    "    gridded_radar_interp_list = []\n",
    "    \n",
    "    # Grab first time from full dataset and restore singular time dimension\n",
    "    first_time_ds = gridded_radar.isel(time=0)\n",
    "    first_time_ds = first_time_ds.expand_dims(dim='time')\n",
    "\n",
    "    gridded_radar_interp_list.append(first_time_ds)\n",
    "    \n",
    "#     tbgn = first_time_ds.coords['time_seconds'].values.item()  # Need to get scalar value, not 0-d\n",
    "#                                                                # numpy array\n",
    "    \n",
    "    # Loop through the gridded_radar times, perform advection correction/interpolation between successive times\n",
    "    # and add each to the list, making sure the time coordinate is consistent\n",
    "    # new_time = tbgn\n",
    "    for i, tdiff in enumerate(tdiffs.values):\n",
    "        gridded_radar_interp_sublist = advection_correction_ds(gridded_radar.isel(time=slice(i, i+2)), \n",
    "                                                               tdiff, tinterp_intv, \n",
    "                                                               base_field_name=base_field_name)\n",
    "        for t, gridded_radar_interp in enumerate(gridded_radar_interp_sublist):\n",
    "#             new_time = new_time + tinterp_intv\n",
    "#             new_ds = first_time_ds.copy()\n",
    "#             new_ds[:] = gridded_radar_interp\n",
    "#             new_ds.coords['time'] = new_ds['time'] + np.timedelta64(int(new_time), 's')\n",
    "#             new_ds.coords['time_seconds'] = new_time\n",
    "            gridded_radar_interp_list.append(gridded_radar_interp)\n",
    "    \n",
    "    return gridded_radar_interp_list\n",
    "\n",
    "\n",
    "def advection_correction_ds(radar_ds, tintv_obs, tintv, base_field_name='reflectivity_masked', method=\"LK\"):\n",
    "    # Evaluate advection\n",
    "    oflow_method = motion.get_method(method)\n",
    "    fd_kwargs = {\"buffer_mask\": 10}  # avoid edge effects\n",
    "\n",
    "    base_field = radar_ds[base_field_name]\n",
    "    oflow_field = oflow_method(base_field, fd_kwargs=fd_kwargs)\n",
    "    \n",
    "    # Perform temporal interpolation on all variables in Dataset using the flow field derived from the \"base\"\n",
    "    # field (by default, reflectivity)\n",
    "    \n",
    "    tbgn = base_field[0].coords['time_seconds'].values.item()   # Need to get scalar value, not 0-d\n",
    "                                                                # numpy array\n",
    "    \n",
    "    radar_ds_list = []\n",
    "    x, y = np.meshgrid(\n",
    "        np.arange(base_field[0].shape[1], dtype=float), np.arange(base_field[0].shape[0], dtype=float),\n",
    "    )\n",
    "    \n",
    "    new_time = tbgn\n",
    "    for i in np.arange(tintv, tintv_obs + tintv, tintv):\n",
    "\n",
    "        new_time = new_time + tintv\n",
    "        \n",
    "        pos1 = (y - i / tintv_obs * oflow_field[1], x - i / tintv_obs * oflow_field[0])\n",
    "        pos2 = (y + (tintv_obs - i) / tintv_obs * oflow_field[1], \n",
    "                x + (tintv_obs - i) / tintv_obs * oflow_field[0])\n",
    "        \n",
    "        field_interp_list = []\n",
    "        for field_name, field_da in radar_ds.items():\n",
    "            fieldt1 = map_coordinates(field_da[0], pos1, order=1)\n",
    "            fieldt2 = map_coordinates(field_da[1], pos2, order=1)\n",
    "       \n",
    "            field_interp = field_da.isel(time=[0]).copy()\n",
    "            field_interp[:] = ((tintv_obs - i) * fieldt1 + i * fieldt2) / tintv_obs\n",
    "            field_interp.coords['time'] = field_interp['time'] + np.timedelta64(int(new_time - tbgn), 's')\n",
    "            field_interp.coords['time_seconds'] = new_time\n",
    "            field_interp_list.append(field_interp)\n",
    "        \n",
    "        radar_ds_interp = xr.merge(field_interp_list)\n",
    "        radar_ds_list.append(radar_ds_interp)\n",
    "        \n",
    "    return radar_ds_list\n",
    "\n",
    "\n",
    "def advection_correction(arr, tintv_obs, tintv):\n",
    "    \"\"\"\n",
    "    R = np.array([qpe_previous, qpe_current])\n",
    "    T = time between two observations (5 min)\n",
    "    t = interpolation timestep (1 min)\n",
    "    \"\"\"\n",
    "\n",
    "    # Evaluate advection\n",
    "    oflow_method = motion.get_method(\"LK\")\n",
    "    fd_kwargs = {\"buffer_mask\": 10}  # avoid edge effects\n",
    "    V = oflow_method(arr, fd_kwargs=fd_kwargs)\n",
    "\n",
    "    # Perform temporal interpolation\n",
    "    # arr_d = np.zeros((arr[0].shape))\n",
    "    arr_list = []\n",
    "    x, y = np.meshgrid(\n",
    "        np.arange(arr[0].shape[1], dtype=float), np.arange(arr[0].shape[0], dtype=float),\n",
    "    )\n",
    "    for i in np.arange(tintv, tintv_obs + tintv, tintv):\n",
    "\n",
    "        pos1 = (y - i / tintv_obs * V[1], x - i / tintv_obs * V[0])\n",
    "        R1 = map_coordinates(arr[0], pos1, order=1)\n",
    "        \n",
    "        pos2 = (y + (tintv_obs - i) / tintv_obs * V[1], x + (tintv_obs - i) / tintv_obs * V[0])\n",
    "        R2 = map_coordinates(arr[1], pos2, order=1)\n",
    "\n",
    "        arr_interp = ((tintv_obs - i) * R1 + i * R2) / tintv_obs\n",
    "        arr_list.append(arr_interp)\n",
    "\n",
    "    return arr_list\n",
    "\n",
    "\n",
    "def plot_animation(xplt, yplt, field, clevels, cbarlabel=None, cbarintv=None, cmap='pyart_HomeyerRainbow', \n",
    "                   norm=None, PIPS_list=None, PIPS_xy_list=None, ax=None, ptype='pcolor', axestickintv=10000., \n",
    "                   axeslimits=None):\n",
    "    \n",
    "    if norm is None:\n",
    "        norm = cm.colors.Normalize(vmin=clevels[0], vmax=clevels[-1])\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    else:\n",
    "        fig = ax.get_figure()\n",
    "    ims = []\n",
    "    for i, var in enumerate(var_da):\n",
    "        plotdata = []\n",
    "        time = np.datetime_as_string(var.coords['time'].values, unit='m')  # Ugly, but whatever\n",
    "        \n",
    "        title = ax.text(0.5,1.05,\"Time: {}\".format(time), \n",
    "                        size=plt.rcParams[\"axes.titlesize\"],\n",
    "                        ha=\"center\", transform=ax.transAxes)\n",
    "        plotdata.append(title)\n",
    "        \n",
    "        if ptype == 'pcolor':\n",
    "            ci = ax.pcolormesh(xplt, yplt, var.squeeze(), vmin=clevels[0], vmax=clevels[-1], cmap=cmap, \n",
    "                                     norm=norm)\n",
    "            plotdata.append(ci)\n",
    "        else:\n",
    "            ci = ax.contourf(xplt, yplt, var.squeeze(), levels=clevels, \n",
    "                             cmap=cmap, norm=norm)\n",
    "            plotdata.extend(ci.collections)\n",
    "            \n",
    "        if PIPS_list is not None and PIPS_xy_list is not None:\n",
    "            # Plot PIPS locations\n",
    "            for PIPS, PIPS_xy in zip(PIPS_list, PIPS_xy_list):\n",
    "                PIPS_x = PIPS_xy[0]\n",
    "                PIPS_y = PIPS_xy[1]\n",
    "                ax.plot([PIPS_x], [PIPS_y], 'k*')\n",
    "        if i == 0.:\n",
    "            if cbarintv is None:\n",
    "                cbarintv = clevels[1] - clevels[0]\n",
    "            cbarlevels = ticker.MultipleLocator(base=cbarintv)\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "            fig.colorbar(ci, orientation='vertical', ticks=cbarlevels, cax=cax)\n",
    "            if cbarlabel is not None:\n",
    "                cax.set_ylabel(cbarlabel)\n",
    "            formatter = ticker.FuncFormatter(mtokm)\n",
    "            ax.xaxis.set_major_formatter(formatter)\n",
    "            ax.yaxis.set_major_formatter(formatter)\n",
    "            ax.xaxis.set_major_locator(ticker.MultipleLocator(base=axestickintv))\n",
    "            ax.yaxis.set_major_locator(ticker.MultipleLocator(base=axestickintv))\n",
    "            ax.set_xlabel('km')\n",
    "            ax.set_ylabel('km')\n",
    "            if axeslimits is None:\n",
    "                xmin = xplt[0]\n",
    "                xmax = xplt[-1]\n",
    "                ymin = yplt[0]\n",
    "                ymax = yplt[-1]\n",
    "            else:\n",
    "                xmin, xmax, ymin, ymax = axeslimits\n",
    "            ax.set_xlim(xmin, xmax)\n",
    "            ax.set_ylim(ymin, ymax)\n",
    "            ax.set_aspect('equal')\n",
    "            \n",
    "        ims.append(plotdata)\n",
    "    \n",
    "    ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True,\n",
    "                                    repeat_delay=1000)\n",
    "    plt.close()\n",
    "    return ani\n",
    "\n",
    "\n",
    "def interp_pyart_grid_to_PIPS(grid_ds, PIPS_loc_list):\n",
    "    \"\"\"Interpolate pyart gridded radar to PIPS locations given an xarray dataset with pyart grid info and a list\n",
    "       of PIPS lats, lons, and altitudes. Returns lists of PIPS x, y, and z locations in radar grid coordinates\n",
    "       and a list of xarray Datasets for each grid variable interpolated to each PIPS location.\"\"\"\n",
    "\n",
    "    # TODO: instead of returning lists, maybe make the PIPS name a dimension of a new xarray Dataset\n",
    "    \n",
    "    ctrlat = np.float(grid_ds.origin_latitude)\n",
    "    ctrlon = np.float(grid_ds.origin_longitude)\n",
    "    print(ctrlat, ctrlon)\n",
    "\n",
    "    radar_at_PIPS_list = []\n",
    "    PIPS_x_list = []\n",
    "    PIPS_y_list = []\n",
    "    PIPS_z_list = []\n",
    "\n",
    "    for PIPS_loc in PIPS_locs:\n",
    "        PIPS_lat = PIPS_loc[0]\n",
    "        PIPS_lon = PIPS_loc[1]\n",
    "        PIPS_alt = PIPS_loc[2]\n",
    "        radar_alt = np.float(grid_ds.origin_altitude)\n",
    "        \n",
    "        PIPS_z = PIPS_alt - radar_alt\n",
    "        PIPS_z_list.append(PIPS_z)\n",
    "        # Use this function to get the x and y coords of the PIPS. Note that this will only be correct \n",
    "        # if the radar\n",
    "        # grid was created using the default pyart aeqd projection.\n",
    "        PIPS_x, PIPS_y = pyart.core.geographic_to_cartesian_aeqd(PIPS_lon, PIPS_lat, ctrlon, ctrlat)\n",
    "        PIPS_x = PIPS_x.squeeze().item()\n",
    "        PIPS_y = PIPS_y.squeeze().item()\n",
    "        PIPS_x_list.append(PIPS_x)\n",
    "        PIPS_y_list.append(PIPS_y)\n",
    "        print('PIPS lat, lon, alt: ', PIPS_lat, PIPS_lon, PIPS_alt)\n",
    "        print('PIPS x, y, z: ', PIPS_x, PIPS_y, PIPS_z)\n",
    "        radar_at_PIPS_ds = grid_ds.interp(x=PIPS_x, y=PIPS_y)\n",
    "        radar_at_PIPS_list.append(radar_at_PIPS_ds)\n",
    "\n",
    "    return PIPS_x_list, PIPS_y_list, PIPS_z_list, radar_at_PIPS_list\n",
    "\n",
    "\n",
    "def write_cm1(sounding_path, p_sfc, theta_sfc, qv_sfc, z_snd, theta_snd, qv_snd, u_snd, v_snd):\n",
    "    '''Writes a CM1/COMMAS/WRF format sounding file given the appropriate arrays.\n",
    "       IMPORTANT: Following https://www2.mmm.ucar.edu/people/bryan/cm1/soundings/,\n",
    "       units should be in m for height, hPa for pressure, K for potential temperature, g/kg for \n",
    "       water vapor mixing ratio, and m/s for the u and v wind components. Will use metpy/pint to convert to \n",
    "       these units just to be safe\n",
    "    '''\n",
    "    print()\n",
    "    print('********************')\n",
    "    print()\n",
    "    print('creating CM1 sounding...')\n",
    "    \n",
    "    # Convert units to that needed for cm1/wrf/commas format (will do nothing, of course, if the units are \n",
    "    # already correct)\n",
    "    z_snd = z_snd.to(units('m'))\n",
    "    p_sfc = p_sfc.to(units('hPa'))\n",
    "    theta_sfc = theta_sfc.to(units('K'))\n",
    "    theta_snd = theta_snd.to(units('K'))\n",
    "    qv_sfc = qv_sfc.to(units('g/kg'))\n",
    "    qv_snd = qv_snd.to(units('g/kg'))\n",
    "    u_snd = u_snd.to(units('m/s'))\n",
    "    v_snd = v_snd.to(units('m/s'))\n",
    "    \n",
    "    # Construct the header\n",
    "    header = f'{p_sfc.m:8.8f}    {theta_sfc.m:8.8f}   {qv_sfc.m:8.8f}'\n",
    "\n",
    "    # Stuff the sounding arrays into a single 2D array to prep for the call to np.savetxt()\n",
    "    sounding_arr = np.transpose((z_snd.m, theta_snd.m, qv_snd.m, u_snd.m, v_snd.m))\n",
    "\n",
    "    # Write the sounding to the file\n",
    "    # GOTCHA: have to put in \"comments=''\" otherwise savetxt prepends a \"#\" to the header line...\n",
    "    np.savetxt(sounding_path, sounding_arr, fmt='%8.8f', header=header, comments='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T22:55:49.600277Z",
     "start_time": "2021-01-02T22:55:49.480673Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in the file containing the original gridded radar at the top of the sorting layer\n",
    "\n",
    "# 03/25 case\n",
    "# radar_name = 'KGWX'\n",
    "# radar_type= 'NEXRAD'\n",
    "# date = '0325'\n",
    "# radar_start_datetimestamp = '20170325170500'\n",
    "# radar_end_datetimestamp = '20170325183559'\n",
    "# height = 2000.\n",
    "\n",
    "# For 03/30/22 case (PERiLS-2022 IOP2)\n",
    "radar_name = 'KGWX'\n",
    "radar_type= 'NEXRAD'\n",
    "date = '0330'\n",
    "radar_start_datetimestamp = '20220330220424'\n",
    "radar_end_datetimestamp = '20220331035633'\n",
    "height = 500.\n",
    "\n",
    "# Create datetime objects for start and end times\n",
    "datetime_start = datetime.strptime(radar_start_datetimestamp, '%Y%m%d%H%M%S')\n",
    "datetime_end = datetime.strptime(radar_end_datetimestamp, '%Y%m%d%H%M%S')\n",
    "\n",
    "radar_basedir = \\\n",
    "    '/Users/dawson29/Projects/PERiLS/obsdata/2022/NEXRAD/GRID/IOP2/KGWX'\n",
    "gridded_radar_dir = os.path.join(radar_basedir, 'combined')\n",
    "plot_dir = os.path.join(gridded_radar_dir, 'plots')\n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)\n",
    "\n",
    "radar_start_timestamp = datetime_start.strftime('%Y%m%d%H%M')\n",
    "radar_end_timestamp = datetime_end.strftime('%Y%m%d%H%M')\n",
    "\n",
    "# Read in original gridded radar at top of sorting layer\n",
    "gridded_radar_interp_filename = '{}_{}_{}_z{:d}_gridded_interp_retr.nc'.format(radar_name, radar_start_timestamp,\n",
    "                                                                          radar_end_timestamp, int(height))\n",
    "gridded_radar_interp_filepath = os.path.join(gridded_radar_dir, gridded_radar_interp_filename)\n",
    "gridded_radar_interp_ds = xr.open_dataset(gridded_radar_interp_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T22:55:54.670301Z",
     "start_time": "2021-01-02T22:55:54.287683Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read in PIPS data (just to get lat/lon for now)\n",
    "# deployment = 'IOP1A_D1_2017'\n",
    "# PIPS_list = ['PIPS1A', 'PIPS1B', 'PIPS2B']\n",
    "# PIPS_data_dir = '/Volumes/scr_fast/Projects/VORTEXSE/obsdata/full_PIPS_dataset_RB15'\n",
    "\n",
    "deployment = 'IOP2_033022'\n",
    "PIPS_list = ['PIPS1A', 'PIPS1B', 'PIPS2A', 'PIPS3B']\n",
    "PIPS_data_dir = '/Users/dawson29/Projects/PERiLS/obsdata/2022/PIPS_data/IOP2_033022/netcdf'\n",
    "\n",
    "\n",
    "PIPS_ds_list = []\n",
    "PIPS_locs = []\n",
    "\n",
    "for PIPS in PIPS_list:\n",
    "    PIPS_filename = 'parsivel_combined_{}_{}_60s.nc'.format(deployment, PIPS)\n",
    "    PIPS_filepath = os.path.join(PIPS_data_dir, PIPS_filename)\n",
    "    PIPS_ds = xr.load_dataset(PIPS_filepath)\n",
    "    PIPS_ds_list.append(PIPS_ds)\n",
    "    PIPS_loc = eval(PIPS_ds.location)\n",
    "    PIPS_locs.append(PIPS_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-02T22:55:56.190957Z",
     "start_time": "2021-01-02T22:55:56.125420Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find PIPS x, y location by interpolating to its lat/lon point\n",
    "PIPS_x_list, PIPS_y_list, PIPS_z_list, radar_at_PIPS_ds_list = interp_pyart_grid_to_PIPS(gridded_radar_interp_ds, \n",
    "                                                                                         PIPS_locs)\n",
    "    \n",
    "PIPS_z_mean = np.array(PIPS_z_list).mean()\n",
    "print(PIPS_z_list)\n",
    "print(PIPS_z_mean)\n",
    "PIPS_x_arr = np.array(PIPS_x_list)\n",
    "PIPS_y_arr = np.array(PIPS_y_list)\n",
    "\n",
    "print(PIPS_x_arr, PIPS_y_arr)\n",
    "\n",
    "PIPS_xy_list = [(PIPS_x, PIPS_y) for PIPS_x, PIPS_y in zip(PIPS_x_list, PIPS_y_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract within a small bounding box and for a limited time slice\n",
    "\n",
    "# For 04/30 case\n",
    "# tstart = '2017-04-30T20:00'\n",
    "# tstop = '2017-04-30T21:30'\n",
    "\n",
    "# For 03/27 case\n",
    "# tstart = '2017-03-27T19:30'\n",
    "# tstop = '2017-03-27T21:00'\n",
    "\n",
    "# For 03/25 case\n",
    "# tstart = '2017-03-25T17:05'\n",
    "# tstop = '2017-03-25T18:35'\n",
    "\n",
    "# For PERiLS IOP2 2022 case\n",
    "tstart = '2022-03-30T23:30'\n",
    "tstop = '2022-03-31T01:15'\n",
    "\n",
    "# lat_bgn = 33.0\n",
    "# lat_end = 34.5\n",
    "# lon_bgn = -89.0\n",
    "# lon_end = -88.0\n",
    "\n",
    "# Instead of lat and lon bounds, use projection x and y coordinates and center on the mean location\n",
    "# of the PIPS +/- some range\n",
    "\n",
    "PIPS_x_mean = np.mean(PIPS_x_arr)\n",
    "PIPS_y_mean = np.mean(PIPS_y_arr)\n",
    "\n",
    "print(PIPS_x_mean, PIPS_y_mean)\n",
    "\n",
    "x_hw = 25000.\n",
    "y_hw = 25000.\n",
    "\n",
    "xbgn = PIPS_x_mean - x_hw\n",
    "xend = PIPS_x_mean + x_hw\n",
    "ybgn = PIPS_y_mean - y_hw\n",
    "yend = PIPS_y_mean + y_hw\n",
    "\n",
    "# ibgn = 50\n",
    "# iend = 150\n",
    "# jbgn = 125\n",
    "# jend = 225\n",
    "# level = 2\n",
    "# z_level = gridded_radar.point_z['data'][level, 0, 0]\n",
    "# z_level = 1000.\n",
    "\n",
    "# This old way of doing it was based on a lie. The pyart function grid.to_xarray() was erroneously outputting\n",
    "# the lat and lon coordinates as 1D coordinates dimensioned by y and x, respectively. In reality they should\n",
    "# each be dimensioned by (y, x). This has been fixed in the latest version of pyart.\n",
    "# gridded_radar_interp_ds = gridded_radar_interp_ds.swap_dims({'y': 'lat', 'x': 'lon'})\n",
    "# gridded_radar_subgrid = gridded_radar_interp_ds.sel(lat=slice(lat_bgn, lat_end), lon=slice(lon_bgn, lon_end),\n",
    "#                                                     time=slice(tstart, tstop))\n",
    "gridded_radar_subgrid = gridded_radar_interp_ds.sel(x=slice(xbgn, xend), y=slice(ybgn, yend), \n",
    "                                                    time=slice(tstart, tstop))\n",
    "\n",
    "# gridded_radar_subgrid = gridded_radar_subgrid.squeeze()\n",
    "# gridded_radar_subgrid = gridded_radar_subgrid.transpose('time', 'lat', 'lon')\n",
    "print(gridded_radar_subgrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridded_radar_subgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in sounding file to get low-level wind field and then derive storm-relative wind\n",
    "# Storm motion taken from subjective reflectivity tag tracking using GRLevel2\n",
    "# EDIT: don't need storm motion because it is implicitly handled in time-dependent trajectory model\n",
    "# ustorm = 12.51\n",
    "# vstorm = 12.95\n",
    "\n",
    "# EDIT: setting ustorm, vstorm to 0 to force ground-relative flow\n",
    "ustorm = 0.\n",
    "vstorm = 0.\n",
    "\n",
    "# sounding_dir = '/Volumes/scr_fast/Projects/VORTEXSE/obsdata/2017/soundings'\n",
    "# sounding_filename = 'Courtland_1759.txt'\n",
    "# # sounding_dir = '/Users/dawson29/sshfs_mounts/depot/data/Projects/VORTEXSE/obsdata/2017/soundings/COMP5mb'\n",
    "# # sounding_filename = 'Hollywood_201704301954.cls'\n",
    "# sounding_path = os.path.join(sounding_dir, sounding_filename)\n",
    "# sounding_metadata, sounding_df = readESC(sounding_path)\n",
    "\n",
    "# For PERiLS IOP2 UIUC SONDE4\n",
    "sounding_dir = '/Users/dawson29/sshfs_mounts/depot/data/Projects/PERiLS/obsdata/2022/non-radar_QC_Illinois/20220330_IOP02/SONDE4/sounding/L2'\n",
    "sounding_filename = 'SPC_20220330_SONDE4_2328.txt'\n",
    "sounding_path = os.path.join(sounding_dir, sounding_filename)\n",
    "\n",
    "sounding_df = readsharppy(sounding_path)\n",
    "wind_dir = sounding_df['direction'].values*units.degrees\n",
    "# print(wind_dir)\n",
    "wind_speed_kts = sounding_df['speed'].values*units.knots \n",
    "wind_speed_ms = wind_speed_kts.to(units('m/s'))\n",
    "\n",
    "# print(wind_speed_ms)\n",
    "u, v = wind_components(wind_speed_ms, wind_dir)\n",
    "\n",
    "sounding_df['u'] = u\n",
    "sounding_df['v'] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sounding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the air density from the sounding at the chosen height\n",
    "sounding_df_one_height = sounding_df.loc[sounding_df['height'] == height]\n",
    "T = sounding_df_one_height['temperature'].values * units['degC']\n",
    "Td = sounding_df_one_height['dewpoint'].values * units['degC']\n",
    "p = sounding_df_one_height['pressure'].values * units['hPa']\n",
    "\n",
    "RH = mpcalc.relative_humidity_from_dewpoint(T, Td)\n",
    "qv = mpcalc.mixing_ratio_from_relative_humidity(p, T, RH)\n",
    "rhoa = mpcalc.density(p, T, qv)\n",
    "\n",
    "print(rhoa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump sounding to CM1/COMMAS format\n",
    "cm1_sounding_dir = '/Users/dawson29/Projects/commas_sed_tests/PERiLS_IOP2_2022/'\n",
    "cm1_sounding_filename = 'SPC_20220330_SONDE4_2328.commas'\n",
    "cm1_sounding_path = os.path.join(cm1_sounding_dir, cm1_sounding_filename)\n",
    "\n",
    "z_snd = sounding_df['height']\n",
    "z_AGL_snd = z_snd - z_snd[0]\n",
    "z_AGL_snd = z_AGL_snd.values * units('m')\n",
    "p_snd = sounding_df['pressure'].values * units('hPa')\n",
    "p_sfc = p_snd[0]\n",
    "T_snd = sounding_df['temperature'].values * units('degC')\n",
    "Td_snd = sounding_df['dewpoint'].values * units('degC')\n",
    "\n",
    "theta_snd = mpcalc.potential_temperature(p_snd, T_snd)\n",
    "theta_sfc = theta_snd[0]\n",
    "\n",
    "RH_snd = mpcalc.relative_humidity_from_dewpoint(T_snd, Td_snd)\n",
    "qv_snd = mpcalc.mixing_ratio_from_relative_humidity(p_snd, T_snd, RH_snd)\n",
    "qv_sfc = qv_snd[0]\n",
    "\n",
    "u_snd = sounding_df['u'].values * units('m/s')\n",
    "v_snd = sounding_df['v'].values * units('m/s')\n",
    "\n",
    "write_cm1(cm1_sounding_path, p_sfc, theta_sfc, qv_sfc, z_AGL_snd, theta_snd, qv_snd, u_snd, v_snd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the q, Nt, and Z moments from the gamma distribution parameters\n",
    "\n",
    "retr_suffix = 'Z01_4dB'\n",
    "\n",
    "lamda = gridded_radar_subgrid[f'lamda_{retr_suffix}'] * 1000. # get to m^-1\n",
    "alpha = gridded_radar_subgrid[f'mu_{retr_suffix}']\n",
    "N0 = gridded_radar_subgrid[f'N0_{retr_suffix}'] * 1000**(1 + alpha) # get to m^-4\n",
    "\n",
    "# Already have Nt\n",
    "Ntr = gridded_radar_subgrid[f'Nt_{retr_suffix}']\n",
    "qr = dsd.calc_qr_gamma(rhoa.m, N0, lamda, alpha)\n",
    "Zr = dsd.calc_Zr_lin_gamma(rhoa.m, qr, Ntr, alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a short range of times for testing:\n",
    "\n",
    "tbgn = '2022-03-31T00:00:00'\n",
    "tend = '2022-03-31T00:30:00'\n",
    "\n",
    "qr_tslice = qr.sel(time=slice(tbgn, tend)).fillna(0.0)\n",
    "Ntr_tslice = Ntr.sel(time=slice(tbgn, tend)).fillna(0.0)\n",
    "Zr_tslice = Zr.sel(time=slice(tbgn, tend)).fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_tslice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_tslice.isel(time=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qr_onetime in qr_tslice:\n",
    "    # timestamp = np.datetime_as_string(qr_onetime.coords['time'].values, unit='m')\n",
    "    time = qr_onetime['time'].values\n",
    "    # This is insane. Why can't numpy datetime64 and python datetime objects play nice with each other?\n",
    "    time_dt = time.astype('datetime64[us]').astype(datetime)\n",
    "    timestamp = time_dt.strftime('%Y%m%d%H%M%S')\n",
    "    print(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now dump to unformatted fortran binary files (one per time) for reading into COMMAS\n",
    "from scipy.io import FortranFile\n",
    "\n",
    "output_dir = '/Users/dawson29/Projects/commas_sed_tests/PERiLS_IOP2_2022/top_boundary_rain'\n",
    "\n",
    "# Loop through the times, construct the file name using the time in seconds since the start, and dump to \n",
    "# unformatted binary\n",
    "\n",
    "timesecint_start = int(qr_tslice['time_seconds'][0].values)\n",
    "print(timesecint_start)\n",
    "\n",
    "for qr_onetime, Ntr_onetime, Zr_onetime in zip(qr_tslice, Ntr_tslice, Zr_tslice):\n",
    "    timesecint = int(qr_onetime['time_seconds'].values) - timesecint_start\n",
    "    output_filename = f'test_rain_top_{timesecint:06d}.dat'\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    print(f\"Writing {output_path}!\")\n",
    "    \n",
    "    uff = FortranFile(output_path, 'w')\n",
    "    # Need to cast to float32 since that is the precision COMMAS is expecting\n",
    "    uff.write_record(qr_onetime.squeeze().values.astype(np.float32))\n",
    "    uff.write_record(Ntr_onetime.squeeze().values.astype(np.float32))\n",
    "    uff.write_record(Zr_onetime.squeeze().values.astype(np.float32))\n",
    "    uff.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = FortranFile('test_rain_top.dat', 'r')\n",
    "qr_r = fr.read_reals(float)\n",
    "print(qr_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:pyPIPS]",
   "language": "python",
   "name": "conda-env-pyPIPS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
